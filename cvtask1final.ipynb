{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-02-16T09:51:21.946185Z","iopub.execute_input":"2025-02-16T09:51:21.946508Z","iopub.status.idle":"2025-02-16T09:51:23.165908Z","shell.execute_reply.started":"2025-02-16T09:51:21.946487Z","shell.execute_reply":"2025-02-16T09:51:23.164746Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/t10k-labels-idx1-ubyte\n/kaggle/input/t10k-images-idx3-ubyte\n/kaggle/input/fashion-mnist_test.csv\n/kaggle/input/fashion-mnist_train.csv\n/kaggle/input/train-labels-idx1-ubyte\n/kaggle/input/train-images-idx3-ubyte\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef load_idx_file(file_path):\n    with open(file_path, 'rb') as f:\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n    return data\n\ndef preprocess_images(images):\n    \"\"\"Preprocess images by normalizing and flattening them.\"\"\"\n    images = images.astype(np.float32) / 255.0  # Normalize images to [0, 1] range\n    images = images.reshape(images.shape[0], -1)  # Flatten images\n    return images\n\ndef load_and_preprocess_data(data_path):\n    \"\"\"Load and preprocess dataset.\"\"\"\n    train_images = load_idx_file(os.path.join(data_path, \"train-images-idx3-ubyte\"))[16:].reshape(-1, 28, 28)\n    train_labels = load_idx_file(os.path.join(data_path, \"train-labels-idx1-ubyte\"))[8:]\n    test_images = load_idx_file(os.path.join(data_path, \"t10k-images-idx3-ubyte\"))[16:].reshape(-1, 28, 28)\n    test_labels = load_idx_file(os.path.join(data_path, \"t10k-labels-idx1-ubyte\"))[8:]\n    \n    train_images = preprocess_images(train_images)\n    test_images = preprocess_images(test_images)\n    \n    val_size = int(0.2 * len(train_images))\n    val_images, val_labels = train_images[:val_size], train_labels[:val_size]\n    train_images, train_labels = train_images[val_size:], train_labels[val_size:]\n    \n    return train_images, train_labels, val_images, val_labels, test_images, test_labels\n\ndef create_batches(images, labels, batch_size):\n    indices = np.arange(len(images))\n    np.random.shuffle(indices)\n    for start_idx in range(0, len(images), batch_size):\n        end_idx = min(start_idx + batch_size, len(images))\n        batch_indices = indices[start_idx:end_idx]\n        yield images[batch_indices], labels[batch_indices]\n\nclass MLP:\n    def __init__(self, input_size, layers, activations, initialization=\"random\", dropout=0.0, learning_rate=0.01):\n        self.layers = layers\n        self.activations = activations\n        self.dropout = dropout\n        self.learning_rate = learning_rate\n        self.params = self.initialize_weights(input_size, layers, initialization)\n    \n    def initialize_weights(self, input_size, layers, initialization):\n        params = {}\n        previous_size = input_size\n        for i, layer_size in enumerate(layers):\n            if initialization == \"he\":\n                params[f'W{i}'] = np.random.randn(previous_size, layer_size) * np.sqrt(2 / previous_size)\n            elif initialization == \"glorot\":\n                params[f'W{i}'] = np.random.randn(previous_size, layer_size) * np.sqrt(1 / previous_size)\n            else:\n                params[f'W{i}'] = np.random.randn(previous_size, layer_size) * 0.01\n            params[f'b{i}'] = np.zeros((1, layer_size))\n            # For hidden layers, initialize batch norm parameters\n            if i < len(layers) - 1:\n                params[f'gamma{i}'] = np.ones((1, layer_size))\n                params[f'beta{i}'] = np.zeros((1, layer_size))\n            previous_size = layer_size\n        return params\n\n    def activation(self, x, func):\n        if func == \"relu\":\n            return np.maximum(0, x)\n        elif func == \"leaky_relu\":\n            return np.where(x > 0, x, 0.01 * x)\n        elif func == \"tanh\":\n            return np.tanh(x)\n        elif func == \"gelu\":\n            return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n        # For any unsupported activation, return input as is.\n        return x\n\n    def activation_derivative(self, z, func):\n        if func == \"relu\":\n            return (z > 0).astype(np.float32)\n        elif func == \"leaky_relu\":\n            return np.where(z > 0, 1, 0.01)\n        elif func == \"tanh\":\n            return 1 - np.tanh(z) ** 2\n        elif func == \"gelu\":\n            # Approximate derivative for gelu\n            c = 0.7978845608028654  # sqrt(2/pi)\n            d = 0.044715\n            tanh_out = np.tanh(c * (z + d * np.power(z, 3)))\n            return 0.5 * (1 + tanh_out) + 0.5 * z * (1 - np.power(tanh_out, 2)) * (c + 3 * d * np.power(z, 2))\n        return np.ones_like(z)\n\n    def batchnorm_forward(self, z, layer_index, eps=1e-5):\n        # Compute mean and variance over the batch (axis=0)\n        mu = np.mean(z, axis=0, keepdims=True)\n        var = np.var(z, axis=0, keepdims=True)\n        z_norm = (z - mu) / np.sqrt(var + eps)\n        gamma = self.params[f'gamma{layer_index}']\n        beta = self.params[f'beta{layer_index}']\n        out = gamma * z_norm + beta\n        # Save values needed for backward pass\n        bn_cache = (z, z_norm, mu, var, gamma, beta, eps)\n        return out, bn_cache\n\n    def batchnorm_backward(self, dout, bn_cache):\n        z, z_norm, mu, var, gamma, beta, eps = bn_cache\n        m = z.shape[0]\n        dgamma = np.sum(dout * z_norm, axis=0, keepdims=True)\n        dbeta = np.sum(dout, axis=0, keepdims=True)\n        dz_norm = dout * gamma\n        dvar = np.sum(dz_norm * (z - mu) * -0.5 * np.power(var + eps, -1.5), axis=0, keepdims=True)\n        dmu = np.sum(dz_norm * -1/np.sqrt(var + eps), axis=0, keepdims=True) + dvar * np.mean(-2 * (z - mu), axis=0, keepdims=True)\n        dz = dz_norm / np.sqrt(var + eps) + dvar * 2 * (z - mu) / m + dmu / m\n        return dz, dgamma, dbeta\n\n    def forward(self, x):\n        cache = {\"A0\": x}\n        for i in range(len(self.layers)):\n            # Linear step\n            z_linear = np.dot(cache[f'A{i}'], self.params[f'W{i}']) + self.params[f'b{i}']\n            # For hidden layers, apply batch norm before activation\n            if i < len(self.layers) - 1:\n                z, bn_cache = self.batchnorm_forward(z_linear, i)\n                cache[f'bn{i+1}'] = bn_cache\n            else:\n                z = z_linear\n            cache[f'Z{i+1}'] = z\n            # Activation step\n            a = self.activation(z, self.activations[i])\n            # Apply dropout on hidden layers if dropout > 0\n            if i < len(self.layers) - 1 and self.dropout > 0:\n                dropout_mask = (np.random.rand(*a.shape) > self.dropout) / (1 - self.dropout)\n                a *= dropout_mask\n                cache[f'dropout{i+1}'] = dropout_mask\n            cache[f'A{i+1}'] = a\n        return cache\n\n    def compute_accuracy(self, predictions, labels):\n        return np.mean(np.argmax(predictions, axis=1) == labels)\n\n    def backward(self, cache, labels):\n        m = labels.shape[0]\n        grads = {}\n        # Compute initial gradient from loss (assumes softmax cross-entropy loss)\n        dA = cache[f'A{len(self.layers)}']\n        dA[np.arange(m), labels] -= 1\n        dA /= m\n        \n        for i in reversed(range(len(self.layers))):\n            # If dropout was applied in the forward pass, backpropagate through it.\n            if i < len(self.layers) - 1 and f'dropout{i+1}' in cache:\n                dA = dA * cache[f'dropout{i+1}']\n            # Backprop through activation\n            dZ = dA * self.activation_derivative(cache[f'Z{i+1}'], self.activations[i])\n            # If batch norm was applied, backprop through it\n            if i < len(self.layers) - 1 and f'bn{i+1}' in cache:\n                dZ, dgamma, dbeta = self.batchnorm_backward(dZ, cache[f'bn{i+1}'])\n                grads[f'dgamma{i}'] = dgamma\n                grads[f'dbeta{i}'] = dbeta\n            A_prev = cache[f'A{i}']\n            grads[f'dW{i}'] = np.dot(A_prev.T, dZ)\n            grads[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True)\n            dA = np.dot(dZ, self.params[f'W{i}'].T)\n        return grads\n\n    def update_params(self, grads):\n        for i in range(len(self.layers)):\n            self.params[f'W{i}'] -= self.learning_rate * grads[f'dW{i}']\n            self.params[f'b{i}'] -= self.learning_rate * grads[f'db{i}']\n            if i < len(self.layers) - 1 and f'dgamma{i}' in grads:\n                self.params[f'gamma{i}'] -= self.learning_rate * grads[f'dgamma{i}']\n                self.params[f'beta{i}'] -= self.learning_rate * grads[f'dbeta{i}']\n\n    def compute_loss(self, predictions, labels, epsilon=1e-12):\n        m = labels.shape[0]\n        # Compute softmax probabilities\n        probs = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n        probs /= np.sum(probs, axis=1, keepdims=True)\n        probs = np.clip(probs, epsilon, 1.0)  # Prevent log(0) issues\n        log_likelihood = -np.log(probs[np.arange(m), labels])\n        return np.sum(log_likelihood) / m\n\n    def train(self, train_data, val_data, epochs=10):\n        for epoch in range(epochs):\n            total_loss, total_val_loss = 0, 0\n            total_val_acc = 0\n            for images, labels in train_data:\n                cache = self.forward(images)\n                loss = self.compute_loss(cache[f'A{len(self.layers)}'], labels)\n                grads = self.backward(cache, labels)\n                self.update_params(grads)\n                total_loss += loss\n            \n            for val_images, val_labels in val_data:\n                val_cache = self.forward(val_images)\n                val_loss = self.compute_loss(val_cache[f'A{len(self.layers)}'], val_labels)\n                val_acc = self.compute_accuracy(val_cache[f'A{len(self.layers)}'], val_labels)\n                total_val_loss += val_loss\n                total_val_acc += val_acc\n            \n            avg_train_loss = total_loss / len(train_data)\n            avg_val_loss = total_val_loss / len(val_data)\n            avg_val_acc = total_val_acc / len(val_data)\n            print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_acc:.4f}\")\n\n    \n\n    def evaluate(self, test_data):\n        correct, total = 0, 0\n        all_predictions = []\n        all_labels = []\n    \n        for images, labels in test_data:\n            cache = self.forward(images)\n            predictions = np.argmax(cache[f'A{len(self.layers)}'], axis=1)\n            correct += np.sum(predictions == labels)\n            total += labels.shape[0]\n            \n            all_predictions.extend(predictions)\n            all_labels.extend(labels)\n    \n        accuracy = accuracy_score(all_labels, all_predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n    \n        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n\n\n# Load dataset with preprocessing\ndata_path = \"/kaggle/input/\"\ntrain_images, train_labels, val_images, val_labels, test_images, test_labels = load_and_preprocess_data(data_path)\n\nBATCH_SIZE = 32\ntrain_batches = list(create_batches(train_images, train_labels, BATCH_SIZE))\nval_batches = list(create_batches(val_images, val_labels, BATCH_SIZE))\ntest_batches = list(create_batches(test_images, test_labels, BATCH_SIZE))\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-16T09:51:25.383247Z","iopub.execute_input":"2025-02-16T09:51:25.383721Z","iopub.status.idle":"2025-02-16T09:51:25.665535Z","shell.execute_reply.started":"2025-02-16T09:51:25.383693Z","shell.execute_reply":"2025-02-16T09:51:25.664674Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"## first we try to fix initialisation then other things","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:51:30.643477Z","iopub.execute_input":"2025-02-16T09:51:30.643725Z","iopub.status.idle":"2025-02-16T09:51:30.647066Z","shell.execute_reply.started":"2025-02-16T09:51:30.643708Z","shell.execute_reply":"2025-02-16T09:51:30.646249Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"mlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"he\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"glorot\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:51:32.273128Z","iopub.execute_input":"2025-02-16T09:51:32.273439Z","iopub.status.idle":"2025-02-16T09:59:08.879819Z","shell.execute_reply.started":"2025-02-16T09:51:32.273420Z","shell.execute_reply":"2025-02-16T09:59:08.878661Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 1.8562, Val Loss: 1.8064, Val Accuracy: 0.7913\nEpoch 2, Train Loss: 1.7821, Val Loss: 1.7703, Val Accuracy: 0.8142\nEpoch 3, Train Loss: 1.7578, Val Loss: 1.7492, Val Accuracy: 0.8291\nEpoch 4, Train Loss: 1.7434, Val Loss: 1.7440, Val Accuracy: 0.8315\nEpoch 5, Train Loss: 1.7321, Val Loss: 1.7333, Val Accuracy: 0.8367\nEpoch 6, Train Loss: 1.7241, Val Loss: 1.7273, Val Accuracy: 0.8408\nEpoch 7, Train Loss: 1.7170, Val Loss: 1.7204, Val Accuracy: 0.8453\nEpoch 8, Train Loss: 1.7101, Val Loss: 1.7162, Val Accuracy: 0.8489\nEpoch 9, Train Loss: 1.7053, Val Loss: 1.7087, Val Accuracy: 0.8487\nEpoch 10, Train Loss: 1.7003, Val Loss: 1.7096, Val Accuracy: 0.8539\nEpoch 11, Train Loss: 1.6963, Val Loss: 1.7081, Val Accuracy: 0.8563\nEpoch 12, Train Loss: 1.6931, Val Loss: 1.6991, Val Accuracy: 0.8577\nEpoch 13, Train Loss: 1.6887, Val Loss: 1.6973, Val Accuracy: 0.8588\nEpoch 14, Train Loss: 1.6850, Val Loss: 1.6958, Val Accuracy: 0.8599\nEpoch 15, Train Loss: 1.6819, Val Loss: 1.6975, Val Accuracy: 0.8618\nEpoch 16, Train Loss: 1.6792, Val Loss: 1.6913, Val Accuracy: 0.8632\nEpoch 17, Train Loss: 1.6766, Val Loss: 1.6883, Val Accuracy: 0.8634\nEpoch 18, Train Loss: 1.6735, Val Loss: 1.6874, Val Accuracy: 0.8654\nEpoch 19, Train Loss: 1.6709, Val Loss: 1.6892, Val Accuracy: 0.8660\nEpoch 20, Train Loss: 1.6674, Val Loss: 1.6818, Val Accuracy: 0.8662\nTest Accuracy: 85.61%\nPrecision: 0.85\nRecall: 0.86\nF1 Score: 0.85\nEpoch 1, Train Loss: 1.7460, Val Loss: 1.7163, Val Accuracy: 0.8436\nEpoch 2, Train Loss: 1.6927, Val Loss: 1.6962, Val Accuracy: 0.8524\nEpoch 3, Train Loss: 1.6762, Val Loss: 1.6892, Val Accuracy: 0.8607\nEpoch 4, Train Loss: 1.6650, Val Loss: 1.6778, Val Accuracy: 0.8659\nEpoch 5, Train Loss: 1.6566, Val Loss: 1.6736, Val Accuracy: 0.8660\nEpoch 6, Train Loss: 1.6494, Val Loss: 1.6758, Val Accuracy: 0.8690\nEpoch 7, Train Loss: 1.6431, Val Loss: 1.6639, Val Accuracy: 0.8724\nEpoch 8, Train Loss: 1.6375, Val Loss: 1.6612, Val Accuracy: 0.8706\nEpoch 9, Train Loss: 1.6325, Val Loss: 1.6619, Val Accuracy: 0.8741\nEpoch 10, Train Loss: 1.6284, Val Loss: 1.6656, Val Accuracy: 0.8720\nEpoch 11, Train Loss: 1.6234, Val Loss: 1.6562, Val Accuracy: 0.8726\nEpoch 12, Train Loss: 1.6201, Val Loss: 1.6582, Val Accuracy: 0.8708\nEpoch 13, Train Loss: 1.6154, Val Loss: 1.6546, Val Accuracy: 0.8749\nEpoch 14, Train Loss: 1.6115, Val Loss: 1.6570, Val Accuracy: 0.8726\nEpoch 15, Train Loss: 1.6097, Val Loss: 1.6554, Val Accuracy: 0.8730\nEpoch 16, Train Loss: 1.6059, Val Loss: 1.6480, Val Accuracy: 0.8754\nEpoch 17, Train Loss: 1.6026, Val Loss: 1.6516, Val Accuracy: 0.8748\nEpoch 18, Train Loss: 1.5992, Val Loss: 1.6453, Val Accuracy: 0.8762\nEpoch 19, Train Loss: 1.5970, Val Loss: 1.6460, Val Accuracy: 0.8756\nEpoch 20, Train Loss: 1.5941, Val Loss: 1.6534, Val Accuracy: 0.8727\nTest Accuracy: 86.92%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.8218, Val Loss: 1.7673, Val Accuracy: 0.8090\nEpoch 2, Train Loss: 1.7557, Val Loss: 1.7475, Val Accuracy: 0.8287\nEpoch 3, Train Loss: 1.7335, Val Loss: 1.7351, Val Accuracy: 0.8387\nEpoch 4, Train Loss: 1.7197, Val Loss: 1.7208, Val Accuracy: 0.8480\nEpoch 5, Train Loss: 1.7094, Val Loss: 1.7122, Val Accuracy: 0.8479\nEpoch 6, Train Loss: 1.7014, Val Loss: 1.7063, Val Accuracy: 0.8538\nEpoch 7, Train Loss: 1.6945, Val Loss: 1.7017, Val Accuracy: 0.8583\nEpoch 8, Train Loss: 1.6881, Val Loss: 1.6953, Val Accuracy: 0.8608\nEpoch 9, Train Loss: 1.6820, Val Loss: 1.6926, Val Accuracy: 0.8652\nEpoch 10, Train Loss: 1.6778, Val Loss: 1.6869, Val Accuracy: 0.8614\nEpoch 11, Train Loss: 1.6731, Val Loss: 1.6842, Val Accuracy: 0.8658\nEpoch 12, Train Loss: 1.6695, Val Loss: 1.6775, Val Accuracy: 0.8631\nEpoch 13, Train Loss: 1.6658, Val Loss: 1.6815, Val Accuracy: 0.8681\nEpoch 14, Train Loss: 1.6624, Val Loss: 1.6814, Val Accuracy: 0.8662\nEpoch 15, Train Loss: 1.6579, Val Loss: 1.6801, Val Accuracy: 0.8672\nEpoch 16, Train Loss: 1.6550, Val Loss: 1.6766, Val Accuracy: 0.8674\nEpoch 17, Train Loss: 1.6519, Val Loss: 1.6708, Val Accuracy: 0.8709\nEpoch 18, Train Loss: 1.6493, Val Loss: 1.6743, Val Accuracy: 0.8718\nEpoch 19, Train Loss: 1.6457, Val Loss: 1.6733, Val Accuracy: 0.8716\nEpoch 20, Train Loss: 1.6421, Val Loss: 1.6727, Val Accuracy: 0.8710\nTest Accuracy: 86.37%\nPrecision: 0.86\nRecall: 0.86\nF1 Score: 0.86\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"## now other combinations as required, we fix random as it performed best above","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:59:08.880807Z","iopub.execute_input":"2025-02-16T09:59:08.881056Z","iopub.status.idle":"2025-02-16T09:59:08.884932Z","shell.execute_reply.started":"2025-02-16T09:59:08.881034Z","shell.execute_reply":"2025-02-16T09:59:08.883806Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"mlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.2, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.2, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.05, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.01, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.3, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.4, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.5, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.01)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.001)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.2)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.3)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.4)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"relu\", \"relu\" , \"relu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"leaky_relu\", \"leaky_relu\" , \"leaky_relu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[256, 128, 64, 10], activations=[\"tanh\", \"tanh\" , \"tanh\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[512,256, 128, 64, 10], activations=[\"gelu\",\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[1024,512,256, 128, 64, 10], activations=[\"gelu\",\"gelu\",\"gelu\", \"gelu\" , \"gelu\", \"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[128, 64, 10], activations=[\"gelu\",\"gelu\",\"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)\n\nmlp = MLP(input_size=28*28, layers=[64, 10], activations=[\"gelu\",\"softmax\"], initialization=\"random\", dropout=0.1, learning_rate=0.05)\nmlp.train(train_batches, val_batches, epochs=20)\nmlp.evaluate(test_batches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T10:04:41.237014Z","iopub.execute_input":"2025-02-16T10:04:41.237276Z","iopub.status.idle":"2025-02-16T11:03:36.017930Z","shell.execute_reply.started":"2025-02-16T10:04:41.237258Z","shell.execute_reply":"2025-02-16T11:03:36.017279Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 1.7732, Val Loss: 1.7309, Val Accuracy: 0.8302\nEpoch 2, Train Loss: 1.7200, Val Loss: 1.7188, Val Accuracy: 0.8438\nEpoch 3, Train Loss: 1.7044, Val Loss: 1.7044, Val Accuracy: 0.8502\nEpoch 4, Train Loss: 1.6948, Val Loss: 1.7022, Val Accuracy: 0.8548\nEpoch 5, Train Loss: 1.6874, Val Loss: 1.6932, Val Accuracy: 0.8628\nEpoch 6, Train Loss: 1.6796, Val Loss: 1.6858, Val Accuracy: 0.8606\nEpoch 7, Train Loss: 1.6749, Val Loss: 1.6869, Val Accuracy: 0.8638\nEpoch 8, Train Loss: 1.6698, Val Loss: 1.6879, Val Accuracy: 0.8669\nEpoch 9, Train Loss: 1.6658, Val Loss: 1.6853, Val Accuracy: 0.8681\nEpoch 10, Train Loss: 1.6626, Val Loss: 1.6761, Val Accuracy: 0.8702\nEpoch 11, Train Loss: 1.6598, Val Loss: 1.6772, Val Accuracy: 0.8711\nEpoch 12, Train Loss: 1.6563, Val Loss: 1.6650, Val Accuracy: 0.8720\nEpoch 13, Train Loss: 1.6531, Val Loss: 1.6793, Val Accuracy: 0.8718\nEpoch 14, Train Loss: 1.6503, Val Loss: 1.6791, Val Accuracy: 0.8717\nEpoch 15, Train Loss: 1.6473, Val Loss: 1.6760, Val Accuracy: 0.8712\nEpoch 16, Train Loss: 1.6447, Val Loss: 1.6798, Val Accuracy: 0.8752\nEpoch 17, Train Loss: 1.6428, Val Loss: 1.6746, Val Accuracy: 0.8743\nEpoch 18, Train Loss: 1.6392, Val Loss: 1.6749, Val Accuracy: 0.8722\nEpoch 19, Train Loss: 1.6378, Val Loss: 1.6712, Val Accuracy: 0.8721\nEpoch 20, Train Loss: 1.6357, Val Loss: 1.6715, Val Accuracy: 0.8723\nTest Accuracy: 86.68%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.86\nEpoch 1, Train Loss: 1.7739, Val Loss: 1.7331, Val Accuracy: 0.8318\nEpoch 2, Train Loss: 1.7204, Val Loss: 1.7165, Val Accuracy: 0.8410\nEpoch 3, Train Loss: 1.7038, Val Loss: 1.7074, Val Accuracy: 0.8542\nEpoch 4, Train Loss: 1.6946, Val Loss: 1.7063, Val Accuracy: 0.8554\nEpoch 5, Train Loss: 1.6864, Val Loss: 1.6993, Val Accuracy: 0.8595\nEpoch 6, Train Loss: 1.6811, Val Loss: 1.6899, Val Accuracy: 0.8644\nEpoch 7, Train Loss: 1.6754, Val Loss: 1.6794, Val Accuracy: 0.8642\nEpoch 8, Train Loss: 1.6702, Val Loss: 1.6888, Val Accuracy: 0.8636\nEpoch 9, Train Loss: 1.6668, Val Loss: 1.6759, Val Accuracy: 0.8711\nEpoch 10, Train Loss: 1.6627, Val Loss: 1.6892, Val Accuracy: 0.8703\nEpoch 11, Train Loss: 1.6594, Val Loss: 1.6831, Val Accuracy: 0.8695\nEpoch 12, Train Loss: 1.6560, Val Loss: 1.6872, Val Accuracy: 0.8677\nEpoch 13, Train Loss: 1.6528, Val Loss: 1.6840, Val Accuracy: 0.8656\nEpoch 14, Train Loss: 1.6507, Val Loss: 1.6752, Val Accuracy: 0.8718\nEpoch 15, Train Loss: 1.6482, Val Loss: 1.6765, Val Accuracy: 0.8705\nEpoch 16, Train Loss: 1.6444, Val Loss: 1.6814, Val Accuracy: 0.8746\nEpoch 17, Train Loss: 1.6428, Val Loss: 1.6711, Val Accuracy: 0.8730\nEpoch 18, Train Loss: 1.6400, Val Loss: 1.6803, Val Accuracy: 0.8708\nEpoch 19, Train Loss: 1.6376, Val Loss: 1.6680, Val Accuracy: 0.8707\nEpoch 20, Train Loss: 1.6354, Val Loss: 1.6660, Val Accuracy: 0.8753\nTest Accuracy: 86.61%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.86\nEpoch 1, Train Loss: 1.7463, Val Loss: 1.7012, Val Accuracy: 0.8405\nEpoch 2, Train Loss: 1.6926, Val Loss: 1.6887, Val Accuracy: 0.8537\nEpoch 3, Train Loss: 1.6755, Val Loss: 1.6746, Val Accuracy: 0.8599\nEpoch 4, Train Loss: 1.6645, Val Loss: 1.6695, Val Accuracy: 0.8652\nEpoch 5, Train Loss: 1.6564, Val Loss: 1.6782, Val Accuracy: 0.8642\nEpoch 6, Train Loss: 1.6494, Val Loss: 1.6659, Val Accuracy: 0.8706\nEpoch 7, Train Loss: 1.6432, Val Loss: 1.6593, Val Accuracy: 0.8740\nEpoch 8, Train Loss: 1.6372, Val Loss: 1.6568, Val Accuracy: 0.8759\nEpoch 9, Train Loss: 1.6323, Val Loss: 1.6580, Val Accuracy: 0.8740\nEpoch 10, Train Loss: 1.6276, Val Loss: 1.6588, Val Accuracy: 0.8733\nEpoch 11, Train Loss: 1.6235, Val Loss: 1.6513, Val Accuracy: 0.8719\nEpoch 12, Train Loss: 1.6188, Val Loss: 1.6608, Val Accuracy: 0.8715\nEpoch 13, Train Loss: 1.6146, Val Loss: 1.6523, Val Accuracy: 0.8736\nEpoch 14, Train Loss: 1.6118, Val Loss: 1.6542, Val Accuracy: 0.8737\nEpoch 15, Train Loss: 1.6080, Val Loss: 1.6491, Val Accuracy: 0.8760\nEpoch 16, Train Loss: 1.6046, Val Loss: 1.6514, Val Accuracy: 0.8717\nEpoch 17, Train Loss: 1.6018, Val Loss: 1.6482, Val Accuracy: 0.8755\nEpoch 18, Train Loss: 1.5992, Val Loss: 1.6503, Val Accuracy: 0.8722\nEpoch 19, Train Loss: 1.5968, Val Loss: 1.6455, Val Accuracy: 0.8752\nEpoch 20, Train Loss: 1.5930, Val Loss: 1.6480, Val Accuracy: 0.8752\nTest Accuracy: 87.11%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7340, Val Loss: 1.6946, Val Accuracy: 0.8427\nEpoch 2, Train Loss: 1.6801, Val Loss: 1.6738, Val Accuracy: 0.8551\nEpoch 3, Train Loss: 1.6617, Val Loss: 1.6696, Val Accuracy: 0.8622\nEpoch 4, Train Loss: 1.6493, Val Loss: 1.6618, Val Accuracy: 0.8677\nEpoch 5, Train Loss: 1.6396, Val Loss: 1.6642, Val Accuracy: 0.8660\nEpoch 6, Train Loss: 1.6322, Val Loss: 1.6548, Val Accuracy: 0.8716\nEpoch 7, Train Loss: 1.6247, Val Loss: 1.6508, Val Accuracy: 0.8751\nEpoch 8, Train Loss: 1.6182, Val Loss: 1.6519, Val Accuracy: 0.8777\nEpoch 9, Train Loss: 1.6135, Val Loss: 1.6553, Val Accuracy: 0.8741\nEpoch 10, Train Loss: 1.6074, Val Loss: 1.6484, Val Accuracy: 0.8746\nEpoch 11, Train Loss: 1.6025, Val Loss: 1.6456, Val Accuracy: 0.8756\nEpoch 12, Train Loss: 1.5981, Val Loss: 1.6447, Val Accuracy: 0.8759\nEpoch 13, Train Loss: 1.5938, Val Loss: 1.6443, Val Accuracy: 0.8777\nEpoch 14, Train Loss: 1.5887, Val Loss: 1.6485, Val Accuracy: 0.8766\nEpoch 15, Train Loss: 1.5855, Val Loss: 1.6397, Val Accuracy: 0.8787\nEpoch 16, Train Loss: 1.5822, Val Loss: 1.6456, Val Accuracy: 0.8752\nEpoch 17, Train Loss: 1.5777, Val Loss: 1.6348, Val Accuracy: 0.8761\nEpoch 18, Train Loss: 1.5748, Val Loss: 1.6347, Val Accuracy: 0.8757\nEpoch 19, Train Loss: 1.5713, Val Loss: 1.6341, Val Accuracy: 0.8762\nEpoch 20, Train Loss: 1.5684, Val Loss: 1.6361, Val Accuracy: 0.8750\nTest Accuracy: 87.28%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7230, Val Loss: 1.6840, Val Accuracy: 0.8474\nEpoch 2, Train Loss: 1.6669, Val Loss: 1.6677, Val Accuracy: 0.8622\nEpoch 3, Train Loss: 1.6466, Val Loss: 1.6575, Val Accuracy: 0.8686\nEpoch 4, Train Loss: 1.6327, Val Loss: 1.6490, Val Accuracy: 0.8707\nEpoch 5, Train Loss: 1.6214, Val Loss: 1.6414, Val Accuracy: 0.8746\nEpoch 6, Train Loss: 1.6119, Val Loss: 1.6439, Val Accuracy: 0.8737\nEpoch 7, Train Loss: 1.6036, Val Loss: 1.6363, Val Accuracy: 0.8768\nEpoch 8, Train Loss: 1.5958, Val Loss: 1.6325, Val Accuracy: 0.8782\nEpoch 9, Train Loss: 1.5891, Val Loss: 1.6352, Val Accuracy: 0.8765\nEpoch 10, Train Loss: 1.5823, Val Loss: 1.6331, Val Accuracy: 0.8773\nEpoch 11, Train Loss: 1.5762, Val Loss: 1.6241, Val Accuracy: 0.8779\nEpoch 12, Train Loss: 1.5702, Val Loss: 1.6235, Val Accuracy: 0.8772\nEpoch 13, Train Loss: 1.5644, Val Loss: 1.6196, Val Accuracy: 0.8788\nEpoch 14, Train Loss: 1.5596, Val Loss: 1.6220, Val Accuracy: 0.8792\nEpoch 15, Train Loss: 1.5547, Val Loss: 1.6176, Val Accuracy: 0.8809\nEpoch 16, Train Loss: 1.5503, Val Loss: 1.6175, Val Accuracy: 0.8799\nEpoch 17, Train Loss: 1.5461, Val Loss: 1.6162, Val Accuracy: 0.8797\nEpoch 18, Train Loss: 1.5427, Val Loss: 1.6192, Val Accuracy: 0.8795\nEpoch 19, Train Loss: 1.5390, Val Loss: 1.6199, Val Accuracy: 0.8776\nEpoch 20, Train Loss: 1.5356, Val Loss: 1.6168, Val Accuracy: 0.8803\nTest Accuracy: 87.40%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.8065, Val Loss: 1.7567, Val Accuracy: 0.8149\nEpoch 2, Train Loss: 1.7497, Val Loss: 1.7348, Val Accuracy: 0.8315\nEpoch 3, Train Loss: 1.7346, Val Loss: 1.7370, Val Accuracy: 0.8417\nEpoch 4, Train Loss: 1.7244, Val Loss: 1.7241, Val Accuracy: 0.8488\nEpoch 5, Train Loss: 1.7186, Val Loss: 1.7285, Val Accuracy: 0.8500\nEpoch 6, Train Loss: 1.7119, Val Loss: 1.7204, Val Accuracy: 0.8524\nEpoch 7, Train Loss: 1.7077, Val Loss: 1.7239, Val Accuracy: 0.8508\nEpoch 8, Train Loss: 1.7038, Val Loss: 1.7240, Val Accuracy: 0.8584\nEpoch 9, Train Loss: 1.6992, Val Loss: 1.7046, Val Accuracy: 0.8622\nEpoch 10, Train Loss: 1.6972, Val Loss: 1.7142, Val Accuracy: 0.8573\nEpoch 11, Train Loss: 1.6933, Val Loss: 1.7102, Val Accuracy: 0.8599\nEpoch 12, Train Loss: 1.6905, Val Loss: 1.7027, Val Accuracy: 0.8651\nEpoch 13, Train Loss: 1.6887, Val Loss: 1.7052, Val Accuracy: 0.8648\nEpoch 14, Train Loss: 1.6852, Val Loss: 1.7113, Val Accuracy: 0.8618\nEpoch 15, Train Loss: 1.6834, Val Loss: 1.7041, Val Accuracy: 0.8662\nEpoch 16, Train Loss: 1.6815, Val Loss: 1.6977, Val Accuracy: 0.8672\nEpoch 17, Train Loss: 1.6802, Val Loss: 1.6974, Val Accuracy: 0.8688\nEpoch 18, Train Loss: 1.6777, Val Loss: 1.7058, Val Accuracy: 0.8673\nEpoch 19, Train Loss: 1.6756, Val Loss: 1.6995, Val Accuracy: 0.8671\nEpoch 20, Train Loss: 1.6732, Val Loss: 1.6980, Val Accuracy: 0.8694\nTest Accuracy: 86.00%\nPrecision: 0.86\nRecall: 0.86\nF1 Score: 0.86\nEpoch 1, Train Loss: 1.8437, Val Loss: 1.7994, Val Accuracy: 0.8013\nEpoch 2, Train Loss: 1.7872, Val Loss: 1.7769, Val Accuracy: 0.8209\nEpoch 3, Train Loss: 1.7727, Val Loss: 1.7611, Val Accuracy: 0.8297\nEpoch 4, Train Loss: 1.7623, Val Loss: 1.7593, Val Accuracy: 0.8323\nEpoch 5, Train Loss: 1.7536, Val Loss: 1.7554, Val Accuracy: 0.8390\nEpoch 6, Train Loss: 1.7498, Val Loss: 1.7495, Val Accuracy: 0.8423\nEpoch 7, Train Loss: 1.7448, Val Loss: 1.7518, Val Accuracy: 0.8462\nEpoch 8, Train Loss: 1.7414, Val Loss: 1.7446, Val Accuracy: 0.8478\nEpoch 9, Train Loss: 1.7376, Val Loss: 1.7510, Val Accuracy: 0.8438\nEpoch 10, Train Loss: 1.7340, Val Loss: 1.7503, Val Accuracy: 0.8508\nEpoch 11, Train Loss: 1.7311, Val Loss: 1.7470, Val Accuracy: 0.8516\nEpoch 12, Train Loss: 1.7283, Val Loss: 1.7443, Val Accuracy: 0.8529\nEpoch 13, Train Loss: 1.7261, Val Loss: 1.7345, Val Accuracy: 0.8582\nEpoch 14, Train Loss: 1.7236, Val Loss: 1.7381, Val Accuracy: 0.8601\nEpoch 15, Train Loss: 1.7220, Val Loss: 1.7431, Val Accuracy: 0.8571\nEpoch 16, Train Loss: 1.7200, Val Loss: 1.7340, Val Accuracy: 0.8597\nEpoch 17, Train Loss: 1.7185, Val Loss: 1.7333, Val Accuracy: 0.8583\nEpoch 18, Train Loss: 1.7167, Val Loss: 1.7353, Val Accuracy: 0.8585\nEpoch 19, Train Loss: 1.7132, Val Loss: 1.7371, Val Accuracy: 0.8610\nEpoch 20, Train Loss: 1.7126, Val Loss: 1.7333, Val Accuracy: 0.8608\nTest Accuracy: 85.48%\nPrecision: 0.85\nRecall: 0.85\nF1 Score: 0.85\nEpoch 1, Train Loss: 1.8893, Val Loss: 1.8445, Val Accuracy: 0.7744\nEpoch 2, Train Loss: 1.8341, Val Loss: 1.8214, Val Accuracy: 0.7879\nEpoch 3, Train Loss: 1.8176, Val Loss: 1.8054, Val Accuracy: 0.8067\nEpoch 4, Train Loss: 1.8070, Val Loss: 1.8158, Val Accuracy: 0.8134\nEpoch 5, Train Loss: 1.7984, Val Loss: 1.8079, Val Accuracy: 0.8192\nEpoch 6, Train Loss: 1.7932, Val Loss: 1.7937, Val Accuracy: 0.8296\nEpoch 7, Train Loss: 1.7877, Val Loss: 1.7834, Val Accuracy: 0.8290\nEpoch 8, Train Loss: 1.7847, Val Loss: 1.7947, Val Accuracy: 0.8356\nEpoch 9, Train Loss: 1.7801, Val Loss: 1.7846, Val Accuracy: 0.8337\nEpoch 10, Train Loss: 1.7772, Val Loss: 1.7927, Val Accuracy: 0.8363\nEpoch 11, Train Loss: 1.7758, Val Loss: 1.7808, Val Accuracy: 0.8392\nEpoch 12, Train Loss: 1.7717, Val Loss: 1.7803, Val Accuracy: 0.8395\nEpoch 13, Train Loss: 1.7705, Val Loss: 1.7773, Val Accuracy: 0.8428\nEpoch 14, Train Loss: 1.7688, Val Loss: 1.7825, Val Accuracy: 0.8462\nEpoch 15, Train Loss: 1.7654, Val Loss: 1.7802, Val Accuracy: 0.8463\nEpoch 16, Train Loss: 1.7635, Val Loss: 1.7733, Val Accuracy: 0.8524\nEpoch 17, Train Loss: 1.7642, Val Loss: 1.7689, Val Accuracy: 0.8433\nEpoch 18, Train Loss: 1.7606, Val Loss: 1.7750, Val Accuracy: 0.8508\nEpoch 19, Train Loss: 1.7607, Val Loss: 1.7747, Val Accuracy: 0.8492\nEpoch 20, Train Loss: 1.7592, Val Loss: 1.7721, Val Accuracy: 0.8540\nTest Accuracy: 84.10%\nPrecision: 0.84\nRecall: 0.84\nF1 Score: 0.84\nEpoch 1, Train Loss: 1.7771, Val Loss: 1.7233, Val Accuracy: 0.8348\nEpoch 2, Train Loss: 1.7072, Val Loss: 1.7038, Val Accuracy: 0.8502\nEpoch 3, Train Loss: 1.6873, Val Loss: 1.6940, Val Accuracy: 0.8602\nEpoch 4, Train Loss: 1.6755, Val Loss: 1.6849, Val Accuracy: 0.8632\nEpoch 5, Train Loss: 1.6672, Val Loss: 1.6767, Val Accuracy: 0.8635\nEpoch 6, Train Loss: 1.6599, Val Loss: 1.6692, Val Accuracy: 0.8682\nEpoch 7, Train Loss: 1.6538, Val Loss: 1.6703, Val Accuracy: 0.8691\nEpoch 8, Train Loss: 1.6484, Val Loss: 1.6701, Val Accuracy: 0.8703\nEpoch 9, Train Loss: 1.6436, Val Loss: 1.6651, Val Accuracy: 0.8697\nEpoch 10, Train Loss: 1.6388, Val Loss: 1.6659, Val Accuracy: 0.8723\nEpoch 11, Train Loss: 1.6354, Val Loss: 1.6637, Val Accuracy: 0.8726\nEpoch 12, Train Loss: 1.6314, Val Loss: 1.6589, Val Accuracy: 0.8758\nEpoch 13, Train Loss: 1.6280, Val Loss: 1.6582, Val Accuracy: 0.8724\nEpoch 14, Train Loss: 1.6249, Val Loss: 1.6626, Val Accuracy: 0.8712\nEpoch 15, Train Loss: 1.6206, Val Loss: 1.6558, Val Accuracy: 0.8748\nEpoch 16, Train Loss: 1.6182, Val Loss: 1.6565, Val Accuracy: 0.8745\nEpoch 17, Train Loss: 1.6148, Val Loss: 1.6568, Val Accuracy: 0.8737\nEpoch 18, Train Loss: 1.6121, Val Loss: 1.6557, Val Accuracy: 0.8778\nEpoch 19, Train Loss: 1.6098, Val Loss: 1.6561, Val Accuracy: 0.8740\nEpoch 20, Train Loss: 1.6066, Val Loss: 1.6552, Val Accuracy: 0.8758\nTest Accuracy: 86.87%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.9706, Val Loss: 1.8350, Val Accuracy: 0.7642\nEpoch 2, Train Loss: 1.8026, Val Loss: 1.7791, Val Accuracy: 0.8023\nEpoch 3, Train Loss: 1.7651, Val Loss: 1.7546, Val Accuracy: 0.8188\nEpoch 4, Train Loss: 1.7470, Val Loss: 1.7400, Val Accuracy: 0.8298\nEpoch 5, Train Loss: 1.7340, Val Loss: 1.7306, Val Accuracy: 0.8393\nEpoch 6, Train Loss: 1.7246, Val Loss: 1.7234, Val Accuracy: 0.8377\nEpoch 7, Train Loss: 1.7169, Val Loss: 1.7173, Val Accuracy: 0.8440\nEpoch 8, Train Loss: 1.7097, Val Loss: 1.7137, Val Accuracy: 0.8463\nEpoch 9, Train Loss: 1.7055, Val Loss: 1.7085, Val Accuracy: 0.8489\nEpoch 10, Train Loss: 1.7005, Val Loss: 1.7035, Val Accuracy: 0.8522\nEpoch 11, Train Loss: 1.6960, Val Loss: 1.7008, Val Accuracy: 0.8522\nEpoch 12, Train Loss: 1.6922, Val Loss: 1.6978, Val Accuracy: 0.8565\nEpoch 13, Train Loss: 1.6897, Val Loss: 1.6945, Val Accuracy: 0.8566\nEpoch 14, Train Loss: 1.6860, Val Loss: 1.6926, Val Accuracy: 0.8587\nEpoch 15, Train Loss: 1.6825, Val Loss: 1.6912, Val Accuracy: 0.8592\nEpoch 16, Train Loss: 1.6794, Val Loss: 1.6880, Val Accuracy: 0.8604\nEpoch 17, Train Loss: 1.6779, Val Loss: 1.6872, Val Accuracy: 0.8607\nEpoch 18, Train Loss: 1.6747, Val Loss: 1.6857, Val Accuracy: 0.8627\nEpoch 19, Train Loss: 1.6721, Val Loss: 1.6827, Val Accuracy: 0.8670\nEpoch 20, Train Loss: 1.6706, Val Loss: 1.6813, Val Accuracy: 0.8665\nTest Accuracy: 85.32%\nPrecision: 0.85\nRecall: 0.85\nF1 Score: 0.85\nEpoch 1, Train Loss: 1.7556, Val Loss: 1.7220, Val Accuracy: 0.8297\nEpoch 2, Train Loss: 1.6978, Val Loss: 1.7070, Val Accuracy: 0.8444\nEpoch 3, Train Loss: 1.6786, Val Loss: 1.6956, Val Accuracy: 0.8540\nEpoch 4, Train Loss: 1.6669, Val Loss: 1.6744, Val Accuracy: 0.8617\nEpoch 5, Train Loss: 1.6572, Val Loss: 1.6826, Val Accuracy: 0.8628\nEpoch 6, Train Loss: 1.6497, Val Loss: 1.6734, Val Accuracy: 0.8612\nEpoch 7, Train Loss: 1.6429, Val Loss: 1.6704, Val Accuracy: 0.8683\nEpoch 8, Train Loss: 1.6368, Val Loss: 1.6624, Val Accuracy: 0.8674\nEpoch 9, Train Loss: 1.6307, Val Loss: 1.6620, Val Accuracy: 0.8700\nEpoch 10, Train Loss: 1.6245, Val Loss: 1.6624, Val Accuracy: 0.8716\nEpoch 11, Train Loss: 1.6209, Val Loss: 1.6681, Val Accuracy: 0.8708\nEpoch 12, Train Loss: 1.6159, Val Loss: 1.6621, Val Accuracy: 0.8724\nEpoch 13, Train Loss: 1.6109, Val Loss: 1.6630, Val Accuracy: 0.8737\nEpoch 14, Train Loss: 1.6077, Val Loss: 1.6566, Val Accuracy: 0.8752\nEpoch 15, Train Loss: 1.6039, Val Loss: 1.6562, Val Accuracy: 0.8740\nEpoch 16, Train Loss: 1.5998, Val Loss: 1.6556, Val Accuracy: 0.8734\nEpoch 17, Train Loss: 1.5964, Val Loss: 1.6591, Val Accuracy: 0.8757\nEpoch 18, Train Loss: 1.5932, Val Loss: 1.6653, Val Accuracy: 0.8715\nEpoch 19, Train Loss: 1.5896, Val Loss: 1.6570, Val Accuracy: 0.8731\nEpoch 20, Train Loss: 1.5860, Val Loss: 1.6542, Val Accuracy: 0.8728\nTest Accuracy: 86.88%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7677, Val Loss: 1.7193, Val Accuracy: 0.8304\nEpoch 2, Train Loss: 1.7023, Val Loss: 1.6947, Val Accuracy: 0.8459\nEpoch 3, Train Loss: 1.6822, Val Loss: 1.6737, Val Accuracy: 0.8555\nEpoch 4, Train Loss: 1.6693, Val Loss: 1.6781, Val Accuracy: 0.8609\nEpoch 5, Train Loss: 1.6594, Val Loss: 1.6635, Val Accuracy: 0.8655\nEpoch 6, Train Loss: 1.6505, Val Loss: 1.6621, Val Accuracy: 0.8662\nEpoch 7, Train Loss: 1.6429, Val Loss: 1.6648, Val Accuracy: 0.8678\nEpoch 8, Train Loss: 1.6369, Val Loss: 1.6539, Val Accuracy: 0.8698\nEpoch 9, Train Loss: 1.6306, Val Loss: 1.6380, Val Accuracy: 0.8708\nEpoch 10, Train Loss: 1.6248, Val Loss: 1.6467, Val Accuracy: 0.8736\nEpoch 11, Train Loss: 1.6202, Val Loss: 1.6435, Val Accuracy: 0.8715\nEpoch 12, Train Loss: 1.6152, Val Loss: 1.6411, Val Accuracy: 0.8766\nEpoch 13, Train Loss: 1.6109, Val Loss: 1.6488, Val Accuracy: 0.8719\nEpoch 14, Train Loss: 1.6066, Val Loss: 1.6355, Val Accuracy: 0.8746\nEpoch 15, Train Loss: 1.6019, Val Loss: 1.6602, Val Accuracy: 0.8731\nEpoch 16, Train Loss: 1.5986, Val Loss: 1.6463, Val Accuracy: 0.8737\nEpoch 17, Train Loss: 1.5941, Val Loss: 1.6395, Val Accuracy: 0.8756\nEpoch 18, Train Loss: 1.5908, Val Loss: 1.6410, Val Accuracy: 0.8736\nEpoch 19, Train Loss: 1.5865, Val Loss: 1.6349, Val Accuracy: 0.8739\nEpoch 20, Train Loss: 1.5840, Val Loss: 1.6514, Val Accuracy: 0.8738\nTest Accuracy: 87.06%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.8165, Val Loss: 1.7376, Val Accuracy: 0.8143\nEpoch 2, Train Loss: 1.7237, Val Loss: 1.7102, Val Accuracy: 0.8380\nEpoch 3, Train Loss: 1.6991, Val Loss: 1.6897, Val Accuracy: 0.8487\nEpoch 4, Train Loss: 1.6830, Val Loss: 1.6753, Val Accuracy: 0.8560\nEpoch 5, Train Loss: 1.6718, Val Loss: 1.6752, Val Accuracy: 0.8574\nEpoch 6, Train Loss: 1.6621, Val Loss: 1.6732, Val Accuracy: 0.8608\nEpoch 7, Train Loss: 1.6541, Val Loss: 1.6575, Val Accuracy: 0.8688\nEpoch 8, Train Loss: 1.6468, Val Loss: 1.6550, Val Accuracy: 0.8707\nEpoch 9, Train Loss: 1.6396, Val Loss: 1.6653, Val Accuracy: 0.8687\nEpoch 10, Train Loss: 1.6348, Val Loss: 1.6537, Val Accuracy: 0.8693\nEpoch 11, Train Loss: 1.6292, Val Loss: 1.6534, Val Accuracy: 0.8671\nEpoch 12, Train Loss: 1.6239, Val Loss: 1.6489, Val Accuracy: 0.8665\nEpoch 13, Train Loss: 1.6194, Val Loss: 1.6432, Val Accuracy: 0.8688\nEpoch 14, Train Loss: 1.6148, Val Loss: 1.6292, Val Accuracy: 0.8701\nEpoch 15, Train Loss: 1.6103, Val Loss: 1.6485, Val Accuracy: 0.8760\nEpoch 16, Train Loss: 1.6064, Val Loss: 1.6363, Val Accuracy: 0.8720\nEpoch 17, Train Loss: 1.6028, Val Loss: 1.6446, Val Accuracy: 0.8706\nEpoch 18, Train Loss: 1.5982, Val Loss: 1.6293, Val Accuracy: 0.8726\nEpoch 19, Train Loss: 1.5945, Val Loss: 1.6342, Val Accuracy: 0.8730\nEpoch 20, Train Loss: 1.5920, Val Loss: 1.6259, Val Accuracy: 0.8722\nTest Accuracy: 86.73%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7470, Val Loss: 1.7104, Val Accuracy: 0.8393\nEpoch 2, Train Loss: 1.6927, Val Loss: 1.6903, Val Accuracy: 0.8533\nEpoch 3, Train Loss: 1.6759, Val Loss: 1.6763, Val Accuracy: 0.8593\nEpoch 4, Train Loss: 1.6647, Val Loss: 1.6701, Val Accuracy: 0.8630\nEpoch 5, Train Loss: 1.6564, Val Loss: 1.6739, Val Accuracy: 0.8661\nEpoch 6, Train Loss: 1.6483, Val Loss: 1.6666, Val Accuracy: 0.8710\nEpoch 7, Train Loss: 1.6430, Val Loss: 1.6597, Val Accuracy: 0.8730\nEpoch 8, Train Loss: 1.6366, Val Loss: 1.6595, Val Accuracy: 0.8700\nEpoch 9, Train Loss: 1.6323, Val Loss: 1.6646, Val Accuracy: 0.8718\nEpoch 10, Train Loss: 1.6286, Val Loss: 1.6660, Val Accuracy: 0.8715\nEpoch 11, Train Loss: 1.6241, Val Loss: 1.6571, Val Accuracy: 0.8744\nEpoch 12, Train Loss: 1.6201, Val Loss: 1.6545, Val Accuracy: 0.8731\nEpoch 13, Train Loss: 1.6164, Val Loss: 1.6551, Val Accuracy: 0.8754\nEpoch 14, Train Loss: 1.6122, Val Loss: 1.6501, Val Accuracy: 0.8768\nEpoch 15, Train Loss: 1.6095, Val Loss: 1.6519, Val Accuracy: 0.8768\nEpoch 16, Train Loss: 1.6059, Val Loss: 1.6539, Val Accuracy: 0.8728\nEpoch 17, Train Loss: 1.6027, Val Loss: 1.6538, Val Accuracy: 0.8773\nEpoch 18, Train Loss: 1.6001, Val Loss: 1.6576, Val Accuracy: 0.8750\nEpoch 19, Train Loss: 1.5974, Val Loss: 1.6554, Val Accuracy: 0.8750\nEpoch 20, Train Loss: 1.5937, Val Loss: 1.6512, Val Accuracy: 0.8691\nTest Accuracy: 87.24%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7476, Val Loss: 1.7121, Val Accuracy: 0.8331\nEpoch 2, Train Loss: 1.6924, Val Loss: 1.6940, Val Accuracy: 0.8464\nEpoch 3, Train Loss: 1.6760, Val Loss: 1.6864, Val Accuracy: 0.8574\nEpoch 4, Train Loss: 1.6647, Val Loss: 1.6773, Val Accuracy: 0.8601\nEpoch 5, Train Loss: 1.6558, Val Loss: 1.6673, Val Accuracy: 0.8646\nEpoch 6, Train Loss: 1.6494, Val Loss: 1.6649, Val Accuracy: 0.8684\nEpoch 7, Train Loss: 1.6433, Val Loss: 1.6670, Val Accuracy: 0.8681\nEpoch 8, Train Loss: 1.6375, Val Loss: 1.6695, Val Accuracy: 0.8689\nEpoch 9, Train Loss: 1.6337, Val Loss: 1.6511, Val Accuracy: 0.8707\nEpoch 10, Train Loss: 1.6283, Val Loss: 1.6582, Val Accuracy: 0.8728\nEpoch 11, Train Loss: 1.6247, Val Loss: 1.6604, Val Accuracy: 0.8731\nEpoch 12, Train Loss: 1.6202, Val Loss: 1.6516, Val Accuracy: 0.8706\nEpoch 13, Train Loss: 1.6169, Val Loss: 1.6636, Val Accuracy: 0.8722\nEpoch 14, Train Loss: 1.6138, Val Loss: 1.6530, Val Accuracy: 0.8728\nEpoch 15, Train Loss: 1.6100, Val Loss: 1.6482, Val Accuracy: 0.8728\nEpoch 16, Train Loss: 1.6069, Val Loss: 1.6494, Val Accuracy: 0.8730\nEpoch 17, Train Loss: 1.6044, Val Loss: 1.6503, Val Accuracy: 0.8726\nEpoch 18, Train Loss: 1.6010, Val Loss: 1.6543, Val Accuracy: 0.8715\nEpoch 19, Train Loss: 1.5979, Val Loss: 1.6484, Val Accuracy: 0.8703\nEpoch 20, Train Loss: 1.5955, Val Loss: 1.6539, Val Accuracy: 0.8699\nTest Accuracy: 86.73%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7485, Val Loss: 1.7055, Val Accuracy: 0.8347\nEpoch 2, Train Loss: 1.6939, Val Loss: 1.6968, Val Accuracy: 0.8419\nEpoch 3, Train Loss: 1.6762, Val Loss: 1.6813, Val Accuracy: 0.8588\nEpoch 4, Train Loss: 1.6656, Val Loss: 1.6769, Val Accuracy: 0.8592\nEpoch 5, Train Loss: 1.6573, Val Loss: 1.6721, Val Accuracy: 0.8608\nEpoch 6, Train Loss: 1.6492, Val Loss: 1.6615, Val Accuracy: 0.8668\nEpoch 7, Train Loss: 1.6436, Val Loss: 1.6625, Val Accuracy: 0.8696\nEpoch 8, Train Loss: 1.6382, Val Loss: 1.6581, Val Accuracy: 0.8687\nEpoch 9, Train Loss: 1.6334, Val Loss: 1.6533, Val Accuracy: 0.8730\nEpoch 10, Train Loss: 1.6287, Val Loss: 1.6552, Val Accuracy: 0.8718\nEpoch 11, Train Loss: 1.6249, Val Loss: 1.6563, Val Accuracy: 0.8717\nEpoch 12, Train Loss: 1.6209, Val Loss: 1.6524, Val Accuracy: 0.8708\nEpoch 13, Train Loss: 1.6174, Val Loss: 1.6523, Val Accuracy: 0.8718\nEpoch 14, Train Loss: 1.6130, Val Loss: 1.6547, Val Accuracy: 0.8706\nEpoch 15, Train Loss: 1.6100, Val Loss: 1.6452, Val Accuracy: 0.8709\nEpoch 16, Train Loss: 1.6073, Val Loss: 1.6504, Val Accuracy: 0.8718\nEpoch 17, Train Loss: 1.6037, Val Loss: 1.6447, Val Accuracy: 0.8743\nEpoch 18, Train Loss: 1.6009, Val Loss: 1.6401, Val Accuracy: 0.8740\nEpoch 19, Train Loss: 1.5989, Val Loss: 1.6481, Val Accuracy: 0.8739\nEpoch 20, Train Loss: 1.5958, Val Loss: 1.6487, Val Accuracy: 0.8766\nTest Accuracy: 86.50%\nPrecision: 0.86\nRecall: 0.86\nF1 Score: 0.86\nEpoch 1, Train Loss: 1.7727, Val Loss: 1.7363, Val Accuracy: 0.8224\nEpoch 2, Train Loss: 1.7198, Val Loss: 1.7147, Val Accuracy: 0.8374\nEpoch 3, Train Loss: 1.7024, Val Loss: 1.7047, Val Accuracy: 0.8427\nEpoch 4, Train Loss: 1.6909, Val Loss: 1.6961, Val Accuracy: 0.8518\nEpoch 5, Train Loss: 1.6826, Val Loss: 1.6843, Val Accuracy: 0.8534\nEpoch 6, Train Loss: 1.6763, Val Loss: 1.6847, Val Accuracy: 0.8598\nEpoch 7, Train Loss: 1.6702, Val Loss: 1.6747, Val Accuracy: 0.8612\nEpoch 8, Train Loss: 1.6642, Val Loss: 1.6738, Val Accuracy: 0.8633\nEpoch 9, Train Loss: 1.6596, Val Loss: 1.6685, Val Accuracy: 0.8670\nEpoch 10, Train Loss: 1.6553, Val Loss: 1.6687, Val Accuracy: 0.8653\nEpoch 11, Train Loss: 1.6523, Val Loss: 1.6670, Val Accuracy: 0.8672\nEpoch 12, Train Loss: 1.6479, Val Loss: 1.6671, Val Accuracy: 0.8697\nEpoch 13, Train Loss: 1.6449, Val Loss: 1.6612, Val Accuracy: 0.8688\nEpoch 14, Train Loss: 1.6415, Val Loss: 1.6639, Val Accuracy: 0.8710\nEpoch 15, Train Loss: 1.6383, Val Loss: 1.6625, Val Accuracy: 0.8698\nEpoch 16, Train Loss: 1.6351, Val Loss: 1.6639, Val Accuracy: 0.8696\nEpoch 17, Train Loss: 1.6333, Val Loss: 1.6596, Val Accuracy: 0.8727\nEpoch 18, Train Loss: 1.6301, Val Loss: 1.6558, Val Accuracy: 0.8708\nEpoch 19, Train Loss: 1.6278, Val Loss: 1.6508, Val Accuracy: 0.8708\nEpoch 20, Train Loss: 1.6246, Val Loss: 1.6551, Val Accuracy: 0.8722\nTest Accuracy: 86.51%\nPrecision: 0.86\nRecall: 0.87\nF1 Score: 0.86\nEpoch 1, Train Loss: 1.7421, Val Loss: 1.6992, Val Accuracy: 0.8397\nEpoch 2, Train Loss: 1.6837, Val Loss: 1.6747, Val Accuracy: 0.8551\nEpoch 3, Train Loss: 1.6645, Val Loss: 1.6694, Val Accuracy: 0.8641\nEpoch 4, Train Loss: 1.6511, Val Loss: 1.6617, Val Accuracy: 0.8668\nEpoch 5, Train Loss: 1.6406, Val Loss: 1.6551, Val Accuracy: 0.8712\nEpoch 6, Train Loss: 1.6314, Val Loss: 1.6571, Val Accuracy: 0.8705\nEpoch 7, Train Loss: 1.6240, Val Loss: 1.6485, Val Accuracy: 0.8708\nEpoch 8, Train Loss: 1.6173, Val Loss: 1.6511, Val Accuracy: 0.8738\nEpoch 9, Train Loss: 1.6118, Val Loss: 1.6411, Val Accuracy: 0.8745\nEpoch 10, Train Loss: 1.6048, Val Loss: 1.6448, Val Accuracy: 0.8758\nEpoch 11, Train Loss: 1.5998, Val Loss: 1.6436, Val Accuracy: 0.8732\nEpoch 12, Train Loss: 1.5939, Val Loss: 1.6458, Val Accuracy: 0.8759\nEpoch 13, Train Loss: 1.5893, Val Loss: 1.6363, Val Accuracy: 0.8741\nEpoch 14, Train Loss: 1.5851, Val Loss: 1.6342, Val Accuracy: 0.8743\nEpoch 15, Train Loss: 1.5809, Val Loss: 1.6421, Val Accuracy: 0.8732\nEpoch 16, Train Loss: 1.5765, Val Loss: 1.6389, Val Accuracy: 0.8728\nEpoch 17, Train Loss: 1.5731, Val Loss: 1.6368, Val Accuracy: 0.8763\nEpoch 18, Train Loss: 1.5699, Val Loss: 1.6337, Val Accuracy: 0.8729\nEpoch 19, Train Loss: 1.5661, Val Loss: 1.6372, Val Accuracy: 0.8772\nEpoch 20, Train Loss: 1.5630, Val Loss: 1.6316, Val Accuracy: 0.8762\nTest Accuracy: 87.24%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7397, Val Loss: 1.6933, Val Accuracy: 0.8398\nEpoch 2, Train Loss: 1.6781, Val Loss: 1.6808, Val Accuracy: 0.8554\nEpoch 3, Train Loss: 1.6558, Val Loss: 1.6706, Val Accuracy: 0.8612\nEpoch 4, Train Loss: 1.6415, Val Loss: 1.6621, Val Accuracy: 0.8656\nEpoch 5, Train Loss: 1.6288, Val Loss: 1.6469, Val Accuracy: 0.8714\nEpoch 6, Train Loss: 1.6181, Val Loss: 1.6538, Val Accuracy: 0.8692\nEpoch 7, Train Loss: 1.6095, Val Loss: 1.6428, Val Accuracy: 0.8727\nEpoch 8, Train Loss: 1.6012, Val Loss: 1.6384, Val Accuracy: 0.8722\nEpoch 9, Train Loss: 1.5937, Val Loss: 1.6491, Val Accuracy: 0.8696\nEpoch 10, Train Loss: 1.5865, Val Loss: 1.6338, Val Accuracy: 0.8736\nEpoch 11, Train Loss: 1.5806, Val Loss: 1.6418, Val Accuracy: 0.8692\nEpoch 12, Train Loss: 1.5746, Val Loss: 1.6313, Val Accuracy: 0.8714\nEpoch 13, Train Loss: 1.5699, Val Loss: 1.6259, Val Accuracy: 0.8782\nEpoch 14, Train Loss: 1.5649, Val Loss: 1.6219, Val Accuracy: 0.8783\nEpoch 15, Train Loss: 1.5609, Val Loss: 1.6289, Val Accuracy: 0.8734\nEpoch 16, Train Loss: 1.5562, Val Loss: 1.6271, Val Accuracy: 0.8762\nEpoch 17, Train Loss: 1.5528, Val Loss: 1.6276, Val Accuracy: 0.8765\nEpoch 18, Train Loss: 1.5490, Val Loss: 1.6253, Val Accuracy: 0.8779\nEpoch 19, Train Loss: 1.5459, Val Loss: 1.6270, Val Accuracy: 0.8698\nEpoch 20, Train Loss: 1.5433, Val Loss: 1.6243, Val Accuracy: 0.8759\nTest Accuracy: 87.42%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7567, Val Loss: 1.7205, Val Accuracy: 0.8358\nEpoch 2, Train Loss: 1.7100, Val Loss: 1.7099, Val Accuracy: 0.8452\nEpoch 3, Train Loss: 1.6963, Val Loss: 1.7017, Val Accuracy: 0.8543\nEpoch 4, Train Loss: 1.6865, Val Loss: 1.6989, Val Accuracy: 0.8598\nEpoch 5, Train Loss: 1.6800, Val Loss: 1.6918, Val Accuracy: 0.8618\nEpoch 6, Train Loss: 1.6736, Val Loss: 1.6884, Val Accuracy: 0.8654\nEpoch 7, Train Loss: 1.6698, Val Loss: 1.6925, Val Accuracy: 0.8688\nEpoch 8, Train Loss: 1.6657, Val Loss: 1.6873, Val Accuracy: 0.8691\nEpoch 9, Train Loss: 1.6619, Val Loss: 1.6832, Val Accuracy: 0.8703\nEpoch 10, Train Loss: 1.6590, Val Loss: 1.6857, Val Accuracy: 0.8700\nEpoch 11, Train Loss: 1.6555, Val Loss: 1.6770, Val Accuracy: 0.8744\nEpoch 12, Train Loss: 1.6530, Val Loss: 1.6800, Val Accuracy: 0.8723\nEpoch 13, Train Loss: 1.6498, Val Loss: 1.6751, Val Accuracy: 0.8722\nEpoch 14, Train Loss: 1.6475, Val Loss: 1.6767, Val Accuracy: 0.8724\nEpoch 15, Train Loss: 1.6454, Val Loss: 1.6736, Val Accuracy: 0.8734\nEpoch 16, Train Loss: 1.6433, Val Loss: 1.6693, Val Accuracy: 0.8726\nEpoch 17, Train Loss: 1.6413, Val Loss: 1.6735, Val Accuracy: 0.8708\nEpoch 18, Train Loss: 1.6396, Val Loss: 1.6762, Val Accuracy: 0.8750\nEpoch 19, Train Loss: 1.6371, Val Loss: 1.6691, Val Accuracy: 0.8758\nEpoch 20, Train Loss: 1.6359, Val Loss: 1.6685, Val Accuracy: 0.8773\nTest Accuracy: 87.01%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 1, Train Loss: 1.7822, Val Loss: 1.7574, Val Accuracy: 0.8210\nEpoch 2, Train Loss: 1.7480, Val Loss: 1.7414, Val Accuracy: 0.8352\nEpoch 3, Train Loss: 1.7379, Val Loss: 1.7349, Val Accuracy: 0.8420\nEpoch 4, Train Loss: 1.7321, Val Loss: 1.7295, Val Accuracy: 0.8406\nEpoch 5, Train Loss: 1.7274, Val Loss: 1.7259, Val Accuracy: 0.8459\nEpoch 6, Train Loss: 1.7234, Val Loss: 1.7264, Val Accuracy: 0.8508\nEpoch 7, Train Loss: 1.7210, Val Loss: 1.7269, Val Accuracy: 0.8494\nEpoch 8, Train Loss: 1.7188, Val Loss: 1.7233, Val Accuracy: 0.8531\nEpoch 9, Train Loss: 1.7165, Val Loss: 1.7189, Val Accuracy: 0.8520\nEpoch 10, Train Loss: 1.7144, Val Loss: 1.7207, Val Accuracy: 0.8558\nEpoch 11, Train Loss: 1.7127, Val Loss: 1.7169, Val Accuracy: 0.8571\nEpoch 12, Train Loss: 1.7111, Val Loss: 1.7186, Val Accuracy: 0.8564\nEpoch 13, Train Loss: 1.7096, Val Loss: 1.7179, Val Accuracy: 0.8582\nEpoch 14, Train Loss: 1.7087, Val Loss: 1.7217, Val Accuracy: 0.8568\nEpoch 15, Train Loss: 1.7064, Val Loss: 1.7128, Val Accuracy: 0.8578\nEpoch 16, Train Loss: 1.7053, Val Loss: 1.7186, Val Accuracy: 0.8587\nEpoch 17, Train Loss: 1.7044, Val Loss: 1.7138, Val Accuracy: 0.8587\nEpoch 18, Train Loss: 1.7035, Val Loss: 1.7142, Val Accuracy: 0.8602\nEpoch 19, Train Loss: 1.7023, Val Loss: 1.7113, Val Accuracy: 0.8598\nEpoch 20, Train Loss: 1.7014, Val Loss: 1.7142, Val Accuracy: 0.8605\nTest Accuracy: 85.19%\nPrecision: 0.85\nRecall: 0.85\nF1 Score: 0.85\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}