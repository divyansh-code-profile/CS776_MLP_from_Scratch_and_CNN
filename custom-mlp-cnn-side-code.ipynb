{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:08.987932Z","iopub.execute_input":"2025-02-16T12:07:08.988208Z","iopub.status.idle":"2025-02-16T12:07:09.308990Z","shell.execute_reply.started":"2025-02-16T12:07:08.988184Z","shell.execute_reply":"2025-02-16T12:07:09.308163Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:09.310192Z","iopub.execute_input":"2025-02-16T12:07:09.310637Z","iopub.status.idle":"2025-02-16T12:07:14.597237Z","shell.execute_reply.started":"2025-02-16T12:07:09.310604Z","shell.execute_reply":"2025-02-16T12:07:14.596589Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:14.598651Z","iopub.execute_input":"2025-02-16T12:07:14.599060Z","iopub.status.idle":"2025-02-16T12:07:14.649397Z","shell.execute_reply.started":"2025-02-16T12:07:14.599037Z","shell.execute_reply":"2025-02-16T12:07:14.648621Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\ntransform = transforms.Compose([             \n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:14.650788Z","iopub.execute_input":"2025-02-16T12:07:14.651024Z","iopub.status.idle":"2025-02-16T12:07:14.665271Z","shell.execute_reply.started":"2025-02-16T12:07:14.651004Z","shell.execute_reply":"2025-02-16T12:07:14.664448Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\n\nval_size = int(0.1 * len(train_dataset))\ntrain_size = len(train_dataset) - val_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:14.666080Z","iopub.execute_input":"2025-02-16T12:07:14.666346Z","iopub.status.idle":"2025-02-16T12:07:16.006584Z","shell.execute_reply.started":"2025-02-16T12:07:14.666319Z","shell.execute_reply":"2025-02-16T12:07:16.005757Z"}},"outputs":[{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 26.4M/26.4M [00:00<00:00, 119MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 29.5k/29.5k [00:00<00:00, 4.23MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.42M/4.42M [00:00<00:00, 61.9MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5.15k/5.15k [00:00<00:00, 8.30MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, layers, activations, dropout, learning_rate, flag=False, device = \"cpu\"):\n        super(MLP, self).__init__()\n        self.input_size = input_size\n        self.layers = layers  \n        self.activations = activations \n        self.dropout_rate = dropout\n        self.learning_rate = learning_rate\n        self.flag = flag\n        self.device = device\n        self.build_model()\n\n    def build_model(self):\n        self.weights = nn.ParameterList()\n        self.biases = nn.ParameterList()\n        self.batch_norm_params = []\n        layer_sizes = [self.input_size] + self.layers\n\n        for i in range(len(layer_sizes) - 1):\n            W = nn.Parameter(torch.randn(layer_sizes[i], layer_sizes[i+1]) * math.sqrt(2 / (layer_sizes[i] + layer_sizes[i+1])))\n            b = nn.Parameter(torch.zeros(layer_sizes[i+1]))\n            self.weights.append(W)\n            self.biases.append(b)\n\n            if i < len(layer_sizes) - 2:\n                gamma = nn.Parameter(torch.ones(layer_sizes[i+1]))\n                beta = nn.Parameter(torch.zeros(layer_sizes[i+1]))\n                self.batch_norm_params.append((gamma, beta))\n\n        self.dropout = nn.Dropout(p=self.dropout_rate)\n\n    def batch_norm_forward(self, x, gamma, beta, eps):\n        # Ensure input and parameters are on the same device\n        x = x.to(self.device)\n        gamma = gamma.to(self.device)\n        beta = beta.to(self.device)\n        \n        mean = x.mean(dim=0, keepdim=True)\n        var = x.var(dim=0, keepdim=True, unbiased=False)\n        x_norm = (x - mean) / torch.sqrt(var + eps)\n        out = gamma * x_norm + beta\n        cache = (x, x_norm, mean, var, gamma, beta, eps)\n        return out, cache\n\n    def batch_norm_backward(self, dout, cache):\n        x, x_norm, mean, var, gamma, beta, eps = cache\n        N, D = dout.shape\n\n        dx_norm = dout * gamma\n        dvar = torch.sum(dx_norm * (x - mean) * -0.5 * torch.pow(var + eps, -1.5), dim=0)\n        dmean = torch.sum(dx_norm * -1 / torch.sqrt(var + eps), dim=0) + dvar * torch.mean(-2 * (x - mean), dim=0)\n\n        dx = (dx_norm / torch.sqrt(var + eps)) + (dvar * 2 * (x - mean) / N) + (dmean / N)\n        dgamma = torch.sum(dout * x_norm, dim=0)\n        dbeta = torch.sum(dout, dim=0)\n\n        return dx, dgamma, dbeta\n\n    def forward(self, x):\n        self.a_values = []  \n        self.z_values = []  \n        self.bn_caches = []\n        a = x  \n\n        for i in range(len(self.weights)):\n            W = self.weights[i]\n            b = self.biases[i]\n            \n            z = torch.matmul(a, W) + b\n            self.z_values.append(z)\n            \n            if i < len(self.weights) - 1:\n                gamma, beta = self.batch_norm_params[i]\n                z, bn_cache = self.batch_norm_forward(z, gamma, beta,1e-5)\n                self.bn_caches.append(bn_cache)\n            \n            activation = self.activations[i]\n\n            if i == len(self.weights) - 1 and self.flag == True:\n                a = z\n            else:\n                if activation == \"relu\":\n                    a = self.relu(z)\n                elif activation == \"leaky_relu\":\n                    a = self.leaky_relu(z)\n                elif activation == \"tanh\":\n                    a = self.tanh(z)\n                elif activation == \"gelu\":\n                    a = self.gelu(z)\n                elif activation == \"softmax\":\n                    a = self.softmax(z)\n                else:\n                    raise ValueError(f\"Unsupported activation: {activation}\")\n\n                if i < len(self.weights) - 1:\n                    a = self.dropout(a)\n\n            self.a_values.append(a)\n\n        return a\n\n    def relu(self, z):\n        return torch.maximum(z, torch.zeros_like(z))\n\n    def relu_derivative(self, z):\n        return torch.where(z > 0, torch.ones_like(z), torch.zeros_like(z))\n\n    def leaky_relu(self, z, negative_slope=0.01):\n        return torch.where(z > 0, z, negative_slope * z)\n\n    def leaky_relu_derivative(self, z, negative_slope=0.01):\n        return torch.where(z > 0, torch.ones_like(z), negative_slope * torch.ones_like(z))\n\n    def tanh(self, z):\n        return torch.tanh(z)\n\n    def tanh_derivative(self, z):\n        return 1 - torch.tanh(z) ** 2\n\n    def gelu(self, z):\n        return 0.5 * z * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (z + 0.044715 * torch.pow(z, 3))))\n\n    def gelu_derivative(self, z):\n        tanh_part = torch.tanh(math.sqrt(2.0 / math.pi) * (z + 0.044715 * torch.pow(z, 3)))\n        factor = 0.5 * (1.0 + tanh_part)\n        derivative = factor + (z * (1 - tanh_part ** 2) * (math.sqrt(2.0 / math.pi) + 0.134145 * z ** 2))\n        return derivative\n\n    def softmax(self, z):\n        z_exp = torch.exp(z - torch.max(z, dim=1, keepdim=True)[0])\n        return z_exp / torch.sum(z_exp, dim=1, keepdim=True)\n\n    def softmax_derivative(self, z):\n        s = self.softmax(z)\n        return s * (1 - s)\n\n    def apply_dropout(self, a, rate):\n        if rate > 0:\n            dropout_mask = (torch.rand_like(a) > rate).float()\n            a = dropout_mask * a / (1.0 - rate)\n        return a\n\n    def train_model(self, train_batches, val_batches, epochs):\n        for epoch in range(epochs):\n            self.train()  \n            total_loss = 0\n\n            for x_batch, y_batch in train_batches:\n                x_batch = torch.tensor(x_batch, dtype=torch.float32)\n                y_batch = torch.tensor(y_batch, dtype=torch.long)\n\n                outputs = self.forward(x_batch)\n\n                loss = self.compute_loss(outputs, y_batch)\n                total_loss += loss.item()\n\n                self.backward(x_batch, y_batch)\n\n                with torch.no_grad():\n                    for i in range(len(self.weights)):\n                        self.weights[i] -= self.learning_rate * self.grad_weights[i]\n                        self.biases[i] -= self.learning_rate * self.grad_biases[i]\n\n                    for i, (gamma, beta) in enumerate(self.batch_norm_params):\n                        gamma_grad = self.grad_batch_norm_params[i][0]\n                        beta_grad = self.grad_batch_norm_params[i][1]\n                        gamma -= self.learning_rate * gamma_grad\n                        beta -= self.learning_rate * beta_grad\n\n            avg_loss = total_loss / len(train_batches)\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n\n            val_accuracy = self.evaluate(val_batches)\n            print(f'Validation Accuracy: {val_accuracy:.2f}%\\n')\n\n    def backward(self, x, y):\n        m = y.shape[0]\n\n        self.grad_weights = [torch.zeros_like(W) for W in self.weights]\n        self.grad_biases = [torch.zeros_like(b) for b in self.biases]\n        self.grad_batch_norm_params = []\n\n        a_final = self.a_values[-1]\n        delta = self.loss_derivative(a_final, y)\n\n        for i in reversed(range(len(self.weights))):\n            a_prev = x if i == 0 else self.a_values[i - 1]\n\n            if i < len(self.weights) - 1:\n                delta, dgamma, dbeta = self.batch_norm_backward(delta, self.bn_caches[i])\n                self.grad_batch_norm_params.insert(0, (dgamma, dbeta))\n\n            self.grad_weights[i] = torch.matmul(a_prev.T, delta) / m\n            self.grad_biases[i] = torch.sum(delta, dim=0) / m\n\n            if i != 0:\n                W = self.weights[i]\n                z = self.z_values[i - 1]\n\n                activation = self.activations[i - 1]\n                if activation == \"relu\":\n                    delta = torch.matmul(delta, W.T) * self.relu_derivative(z)\n                elif activation == \"leaky_relu\":\n                    delta = torch.matmul(delta, W.T) * self.leaky_relu_derivative(z)\n                elif activation == \"tanh\":\n                    delta = torch.matmul(delta, W.T) * self.tanh_derivative(z)\n                elif activation == \"gelu\":\n                    delta = torch.matmul(delta, W.T) * self.gelu_derivative(z)\n                elif activation == \"softmax\":\n                    delta = torch.matmul(delta, W.T) * self.softmax_derivative(z)\n                else:\n                    raise ValueError(f\"Unsupported activation: {activation}\")\n\n                delta = delta * self.dropout_derivative(self.a_values[i - 1])\n\n    def dropout_derivative(self, a):\n        return (a != 0).float()\n\n    def loss_derivative(self, a_final, y):\n        y_one_hot = torch.zeros_like(a_final)\n        y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n        return a_final - y_one_hot\n\n    def compute_loss(self, outputs, targets):\n        epsilon = 1e-12\n        outputs = torch.clamp(outputs, epsilon, 1. - epsilon)\n\n        targets_one_hot = torch.zeros_like(outputs)\n        targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)\n\n        loss = -torch.mean(torch.sum(targets_one_hot * torch.log(outputs), dim=1))\n        return loss\n\n    def evaluate(self, test_batches):\n        self.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for x_batch, y_batch in test_batches:\n                x_batch = torch.tensor(x_batch, dtype=torch.float32)\n                y_batch = torch.tensor(y_batch, dtype=torch.long)\n\n                outputs = self.forward(x_batch)\n\n                _, predicted = torch.max(outputs.data, 1)\n\n                total += y_batch.size(0)\n                correct += (predicted == y_batch).sum().item()\n\n        accuracy = 100 * correct / total\n        return accuracy\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:16.007414Z","iopub.execute_input":"2025-02-16T12:07:16.007682Z","iopub.status.idle":"2025-02-16T12:07:16.036873Z","shell.execute_reply.started":"2025-02-16T12:07:16.007661Z","shell.execute_reply":"2025-02-16T12:07:16.036029Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self, kernel_sizes, strides, paddings, init_method=\"random\"):\n        super(CNNModel, self).__init__()\n        self.kernel_sizes = kernel_sizes\n        self.strides = strides\n        self.paddings = paddings\n        self.init_method = init_method\n        self.input_size = 28\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernel_sizes[0], stride=strides[0], padding=paddings[0])\n        self.bn1 = nn.BatchNorm2d(10)\n        \n        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernel_sizes[1], stride=strides[1], padding=paddings[1])\n        self.bn2 = nn.BatchNorm2d(20)\n        \n        self.conv3 = nn.Conv2d(20, 40, kernel_size=kernel_sizes[2], stride=strides[2], padding=paddings[2])\n        self.bn3 = nn.BatchNorm2d(40)\n        \n        self.conv4 = nn.Conv2d(40,64, kernel_size=kernel_sizes[3], stride=strides[3], padding=paddings[3])\n        self.bn4 = nn.BatchNorm2d(64)\n        \n        self.conv5 = nn.Conv2d(64, 128, kernel_size=kernel_sizes[4], stride=strides[4], padding=paddings[4])\n        self.bn5 = nn.BatchNorm2d(128)\n        \n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.op_size = self.calculate_output_size()\n        self.mlp = MLP(\n            input_size=self.op_size * self.op_size * 128,\n            layers=[ 128, 64, 10],\n            activations=[\"relu\", \"relu\", \"softmax\"],\n            dropout=0.1,\n            learning_rate=0.05,\n            flag=True,\n            device = device\n        )\n        self.dropout = nn.Dropout(0.45)\n        \n        self.initialize_weights()\n        \n        \n    def calculate_output_size(self):\n        size = self.input_size\n        \n        for i in range(2):\n            size = ((size + 2 * self.paddings[i] - self.kernel_sizes[i]) // self.strides[i]) + 1\n            size = ((size - 2) // 2) + 1  # Assuming max pooling with kernel size 2 and stride 2\n        size = ((size - 2) // 2) + 1\n        return size\n      \n    \n        return size\n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                if self.init_method == \"xavier\":\n                    nn.init.xavier_uniform_(m.weight)\n                elif self.init_method == \"he\":\n                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n                else:\n                    nn.init.uniform_(m.weight, -0.1, 0.1)\n                \n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.dropout(x)\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = self.dropout(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = self.dropout(x)\n        x = self.pool(x)\n        x = x.view(-1, 128 * self.op_size * self.op_size)\n        x = self.mlp.forward(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:16.037838Z","iopub.execute_input":"2025-02-16T12:07:16.038080Z","iopub.status.idle":"2025-02-16T12:07:16.055093Z","shell.execute_reply.started":"2025-02-16T12:07:16.038057Z","shell.execute_reply":"2025-02-16T12:07:16.054445Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"kernel_sizes = [3, 3, 3, 3, 3]\nstrides = [1, 1, 1, 1, 1]\npaddings = [1, 1, 1, 1, 1]\nmodel = CNNModel(kernel_sizes, strides, paddings, \"he\").to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:16.056832Z","iopub.execute_input":"2025-02-16T12:07:16.057046Z","iopub.status.idle":"2025-02-16T12:07:16.264822Z","shell.execute_reply.started":"2025-02-16T12:07:16.057019Z","shell.execute_reply":"2025-02-16T12:07:16.263972Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer, epoch):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if batch_idx % 100 == 99:\n            print(f'Epoch {epoch}, Batch {batch_idx + 1}, Loss: {running_loss / 100:.6f}')\n            running_loss = 0.0\n\n\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef validate(model, val_loader, criterion):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            val_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            \n            all_predictions.extend(pred.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n\n    val_loss /= len(val_loader.dataset)\n    val_accuracy = 100. * correct / len(val_loader.dataset)\n\n    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n    accuracy = accuracy_score(all_targets, all_predictions)\n\n    print(f'loss: {val_loss:.6f}, Accuracy: {val_accuracy:.2f}%')\n    print(f'Precision: {precision:.2f}')\n    print(f'Recall: {recall:.2f}')\n    print(f'F1 Score: {f1:.2f}')\n\n    return val_loss, val_accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:16.266005Z","iopub.execute_input":"2025-02-16T12:07:16.266283Z","iopub.status.idle":"2025-02-16T12:07:16.837016Z","shell.execute_reply.started":"2025-02-16T12:07:16.266260Z","shell.execute_reply":"2025-02-16T12:07:16.836339Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_epochs = 20\nbest_val_loss = float('inf')\nfor epoch in range(1, num_epochs + 1):\n    train(model, train_loader, criterion, optimizer, epoch)\n    val_loss, val_accuracy = validate(model, val_loader, criterion)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n\nprint('Training complete!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:07:16.837758Z","iopub.execute_input":"2025-02-16T12:07:16.838082Z","iopub.status.idle":"2025-02-16T12:13:56.828373Z","shell.execute_reply.started":"2025-02-16T12:07:16.838062Z","shell.execute_reply":"2025-02-16T12:13:56.827465Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Batch 100, Loss: 1.552390\nEpoch 1, Batch 200, Loss: 0.952110\nEpoch 1, Batch 300, Loss: 0.810496\nEpoch 1, Batch 400, Loss: 0.752942\nEpoch 1, Batch 500, Loss: 0.721206\nEpoch 1, Batch 600, Loss: 0.692704\nEpoch 1, Batch 700, Loss: 0.633869\nEpoch 1, Batch 800, Loss: 0.626900\nEpoch 1, Batch 900, Loss: 0.601603\nEpoch 1, Batch 1000, Loss: 0.596461\nEpoch 1, Batch 1100, Loss: 0.613794\nEpoch 1, Batch 1200, Loss: 0.557662\nEpoch 1, Batch 1300, Loss: 0.525827\nEpoch 1, Batch 1400, Loss: 0.537548\nEpoch 1, Batch 1500, Loss: 0.530760\nEpoch 1, Batch 1600, Loss: 0.505219\nloss: 0.012892, Accuracy: 85.18%\nPrecision: 0.85\nRecall: 0.85\nF1 Score: 0.85\nEpoch 2, Batch 100, Loss: 0.500540\nEpoch 2, Batch 200, Loss: 0.487344\nEpoch 2, Batch 300, Loss: 0.471153\nEpoch 2, Batch 400, Loss: 0.477912\nEpoch 2, Batch 500, Loss: 0.478380\nEpoch 2, Batch 600, Loss: 0.441333\nEpoch 2, Batch 700, Loss: 0.441646\nEpoch 2, Batch 800, Loss: 0.456779\nEpoch 2, Batch 900, Loss: 0.433252\nEpoch 2, Batch 1000, Loss: 0.448515\nEpoch 2, Batch 1100, Loss: 0.441652\nEpoch 2, Batch 1200, Loss: 0.436273\nEpoch 2, Batch 1300, Loss: 0.407240\nEpoch 2, Batch 1400, Loss: 0.406600\nEpoch 2, Batch 1500, Loss: 0.427420\nEpoch 2, Batch 1600, Loss: 0.430032\nloss: 0.010782, Accuracy: 87.45%\nPrecision: 0.87\nRecall: 0.87\nF1 Score: 0.87\nEpoch 3, Batch 100, Loss: 0.398453\nEpoch 3, Batch 200, Loss: 0.392980\nEpoch 3, Batch 300, Loss: 0.412364\nEpoch 3, Batch 400, Loss: 0.395371\nEpoch 3, Batch 500, Loss: 0.407511\nEpoch 3, Batch 600, Loss: 0.389402\nEpoch 3, Batch 700, Loss: 0.389221\nEpoch 3, Batch 800, Loss: 0.390878\nEpoch 3, Batch 900, Loss: 0.363929\nEpoch 3, Batch 1000, Loss: 0.414521\nEpoch 3, Batch 1100, Loss: 0.389579\nEpoch 3, Batch 1200, Loss: 0.370834\nEpoch 3, Batch 1300, Loss: 0.362679\nEpoch 3, Batch 1400, Loss: 0.373771\nEpoch 3, Batch 1500, Loss: 0.375844\nEpoch 3, Batch 1600, Loss: 0.375040\nloss: 0.010006, Accuracy: 87.75%\nPrecision: 0.88\nRecall: 0.88\nF1 Score: 0.87\nEpoch 4, Batch 100, Loss: 0.342377\nEpoch 4, Batch 200, Loss: 0.349772\nEpoch 4, Batch 300, Loss: 0.357106\nEpoch 4, Batch 400, Loss: 0.343015\nEpoch 4, Batch 500, Loss: 0.358482\nEpoch 4, Batch 600, Loss: 0.349720\nEpoch 4, Batch 700, Loss: 0.357557\nEpoch 4, Batch 800, Loss: 0.352638\nEpoch 4, Batch 900, Loss: 0.344327\nEpoch 4, Batch 1000, Loss: 0.365204\nEpoch 4, Batch 1100, Loss: 0.362813\nEpoch 4, Batch 1200, Loss: 0.338217\nEpoch 4, Batch 1300, Loss: 0.371645\nEpoch 4, Batch 1400, Loss: 0.340587\nEpoch 4, Batch 1500, Loss: 0.342841\nEpoch 4, Batch 1600, Loss: 0.361796\nloss: 0.009460, Accuracy: 88.73%\nPrecision: 0.89\nRecall: 0.89\nF1 Score: 0.89\nEpoch 5, Batch 100, Loss: 0.318718\nEpoch 5, Batch 200, Loss: 0.340309\nEpoch 5, Batch 300, Loss: 0.351483\nEpoch 5, Batch 400, Loss: 0.346785\nEpoch 5, Batch 500, Loss: 0.328561\nEpoch 5, Batch 600, Loss: 0.343293\nEpoch 5, Batch 700, Loss: 0.343441\nEpoch 5, Batch 800, Loss: 0.345482\nEpoch 5, Batch 900, Loss: 0.329369\nEpoch 5, Batch 1000, Loss: 0.330816\nEpoch 5, Batch 1100, Loss: 0.333161\nEpoch 5, Batch 1200, Loss: 0.339986\nEpoch 5, Batch 1300, Loss: 0.349874\nEpoch 5, Batch 1400, Loss: 0.330944\nEpoch 5, Batch 1500, Loss: 0.299409\nEpoch 5, Batch 1600, Loss: 0.322872\nloss: 0.009216, Accuracy: 89.30%\nPrecision: 0.89\nRecall: 0.89\nF1 Score: 0.89\nEpoch 6, Batch 100, Loss: 0.306189\nEpoch 6, Batch 200, Loss: 0.321655\nEpoch 6, Batch 300, Loss: 0.320214\nEpoch 6, Batch 400, Loss: 0.334986\nEpoch 6, Batch 500, Loss: 0.312493\nEpoch 6, Batch 600, Loss: 0.324334\nEpoch 6, Batch 700, Loss: 0.322214\nEpoch 6, Batch 800, Loss: 0.292466\nEpoch 6, Batch 900, Loss: 0.331216\nEpoch 6, Batch 1000, Loss: 0.312017\nEpoch 6, Batch 1100, Loss: 0.309261\nEpoch 6, Batch 1200, Loss: 0.314421\nEpoch 6, Batch 1300, Loss: 0.307140\nEpoch 6, Batch 1400, Loss: 0.330411\nEpoch 6, Batch 1500, Loss: 0.317390\nEpoch 6, Batch 1600, Loss: 0.329353\nloss: 0.008538, Accuracy: 90.18%\nPrecision: 0.90\nRecall: 0.90\nF1 Score: 0.90\nEpoch 7, Batch 100, Loss: 0.316888\nEpoch 7, Batch 200, Loss: 0.314557\nEpoch 7, Batch 300, Loss: 0.289142\nEpoch 7, Batch 400, Loss: 0.274638\nEpoch 7, Batch 500, Loss: 0.289423\nEpoch 7, Batch 600, Loss: 0.312751\nEpoch 7, Batch 700, Loss: 0.319742\nEpoch 7, Batch 800, Loss: 0.298281\nEpoch 7, Batch 900, Loss: 0.294023\nEpoch 7, Batch 1000, Loss: 0.290205\nEpoch 7, Batch 1100, Loss: 0.325929\nEpoch 7, Batch 1200, Loss: 0.304505\nEpoch 7, Batch 1300, Loss: 0.298990\nEpoch 7, Batch 1400, Loss: 0.267285\nEpoch 7, Batch 1500, Loss: 0.308365\nEpoch 7, Batch 1600, Loss: 0.297853\nloss: 0.008439, Accuracy: 90.32%\nPrecision: 0.90\nRecall: 0.90\nF1 Score: 0.90\nEpoch 8, Batch 100, Loss: 0.292764\nEpoch 8, Batch 200, Loss: 0.301287\nEpoch 8, Batch 300, Loss: 0.283629\nEpoch 8, Batch 400, Loss: 0.286535\nEpoch 8, Batch 500, Loss: 0.306692\nEpoch 8, Batch 600, Loss: 0.270686\nEpoch 8, Batch 700, Loss: 0.281695\nEpoch 8, Batch 800, Loss: 0.295889\nEpoch 8, Batch 900, Loss: 0.289677\nEpoch 8, Batch 1000, Loss: 0.296678\nEpoch 8, Batch 1100, Loss: 0.306886\nEpoch 8, Batch 1200, Loss: 0.294723\nEpoch 8, Batch 1300, Loss: 0.299908\nEpoch 8, Batch 1400, Loss: 0.283905\nEpoch 8, Batch 1500, Loss: 0.281587\nEpoch 8, Batch 1600, Loss: 0.310383\nloss: 0.008173, Accuracy: 90.52%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.90\nEpoch 9, Batch 100, Loss: 0.289240\nEpoch 9, Batch 200, Loss: 0.287360\nEpoch 9, Batch 300, Loss: 0.296057\nEpoch 9, Batch 400, Loss: 0.289244\nEpoch 9, Batch 500, Loss: 0.262788\nEpoch 9, Batch 600, Loss: 0.283136\nEpoch 9, Batch 700, Loss: 0.291441\nEpoch 9, Batch 800, Loss: 0.280031\nEpoch 9, Batch 900, Loss: 0.267066\nEpoch 9, Batch 1000, Loss: 0.270485\nEpoch 9, Batch 1100, Loss: 0.281745\nEpoch 9, Batch 1200, Loss: 0.270429\nEpoch 9, Batch 1300, Loss: 0.291011\nEpoch 9, Batch 1400, Loss: 0.272175\nEpoch 9, Batch 1500, Loss: 0.296255\nEpoch 9, Batch 1600, Loss: 0.283192\nloss: 0.007930, Accuracy: 90.78%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 10, Batch 100, Loss: 0.285012\nEpoch 10, Batch 200, Loss: 0.279997\nEpoch 10, Batch 300, Loss: 0.275333\nEpoch 10, Batch 400, Loss: 0.252415\nEpoch 10, Batch 500, Loss: 0.271042\nEpoch 10, Batch 600, Loss: 0.267634\nEpoch 10, Batch 700, Loss: 0.277115\nEpoch 10, Batch 800, Loss: 0.269056\nEpoch 10, Batch 900, Loss: 0.268000\nEpoch 10, Batch 1000, Loss: 0.283975\nEpoch 10, Batch 1100, Loss: 0.270485\nEpoch 10, Batch 1200, Loss: 0.275638\nEpoch 10, Batch 1300, Loss: 0.282539\nEpoch 10, Batch 1400, Loss: 0.293752\nEpoch 10, Batch 1500, Loss: 0.269095\nEpoch 10, Batch 1600, Loss: 0.304810\nloss: 0.007910, Accuracy: 90.63%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 11, Batch 100, Loss: 0.265333\nEpoch 11, Batch 200, Loss: 0.267861\nEpoch 11, Batch 300, Loss: 0.280248\nEpoch 11, Batch 400, Loss: 0.253934\nEpoch 11, Batch 500, Loss: 0.260244\nEpoch 11, Batch 600, Loss: 0.250985\nEpoch 11, Batch 700, Loss: 0.267684\nEpoch 11, Batch 800, Loss: 0.290524\nEpoch 11, Batch 900, Loss: 0.266626\nEpoch 11, Batch 1000, Loss: 0.272482\nEpoch 11, Batch 1100, Loss: 0.271274\nEpoch 11, Batch 1200, Loss: 0.246124\nEpoch 11, Batch 1300, Loss: 0.292514\nEpoch 11, Batch 1400, Loss: 0.272892\nEpoch 11, Batch 1500, Loss: 0.264368\nEpoch 11, Batch 1600, Loss: 0.260597\nloss: 0.008277, Accuracy: 90.42%\nPrecision: 0.91\nRecall: 0.90\nF1 Score: 0.90\nEpoch 12, Batch 100, Loss: 0.276455\nEpoch 12, Batch 200, Loss: 0.246917\nEpoch 12, Batch 300, Loss: 0.266310\nEpoch 12, Batch 400, Loss: 0.290659\nEpoch 12, Batch 500, Loss: 0.266102\nEpoch 12, Batch 600, Loss: 0.270810\nEpoch 12, Batch 700, Loss: 0.251397\nEpoch 12, Batch 800, Loss: 0.246291\nEpoch 12, Batch 900, Loss: 0.276230\nEpoch 12, Batch 1000, Loss: 0.259236\nEpoch 12, Batch 1100, Loss: 0.273365\nEpoch 12, Batch 1200, Loss: 0.248424\nEpoch 12, Batch 1300, Loss: 0.263586\nEpoch 12, Batch 1400, Loss: 0.271456\nEpoch 12, Batch 1500, Loss: 0.271337\nEpoch 12, Batch 1600, Loss: 0.255516\nloss: 0.007714, Accuracy: 91.07%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 13, Batch 100, Loss: 0.261491\nEpoch 13, Batch 200, Loss: 0.271151\nEpoch 13, Batch 300, Loss: 0.260437\nEpoch 13, Batch 400, Loss: 0.253499\nEpoch 13, Batch 500, Loss: 0.279828\nEpoch 13, Batch 600, Loss: 0.257140\nEpoch 13, Batch 700, Loss: 0.249652\nEpoch 13, Batch 800, Loss: 0.256120\nEpoch 13, Batch 900, Loss: 0.266296\nEpoch 13, Batch 1000, Loss: 0.251651\nEpoch 13, Batch 1100, Loss: 0.258154\nEpoch 13, Batch 1200, Loss: 0.251703\nEpoch 13, Batch 1300, Loss: 0.265433\nEpoch 13, Batch 1400, Loss: 0.268773\nEpoch 13, Batch 1500, Loss: 0.258450\nEpoch 13, Batch 1600, Loss: 0.256535\nloss: 0.007578, Accuracy: 90.98%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 14, Batch 100, Loss: 0.248469\nEpoch 14, Batch 200, Loss: 0.244379\nEpoch 14, Batch 300, Loss: 0.240180\nEpoch 14, Batch 400, Loss: 0.239323\nEpoch 14, Batch 500, Loss: 0.268809\nEpoch 14, Batch 600, Loss: 0.244579\nEpoch 14, Batch 700, Loss: 0.255602\nEpoch 14, Batch 800, Loss: 0.257058\nEpoch 14, Batch 900, Loss: 0.253041\nEpoch 14, Batch 1000, Loss: 0.238280\nEpoch 14, Batch 1100, Loss: 0.254725\nEpoch 14, Batch 1200, Loss: 0.262889\nEpoch 14, Batch 1300, Loss: 0.277353\nEpoch 14, Batch 1400, Loss: 0.254187\nEpoch 14, Batch 1500, Loss: 0.259497\nEpoch 14, Batch 1600, Loss: 0.248070\nloss: 0.007707, Accuracy: 90.98%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 15, Batch 100, Loss: 0.254697\nEpoch 15, Batch 200, Loss: 0.249175\nEpoch 15, Batch 300, Loss: 0.252088\nEpoch 15, Batch 400, Loss: 0.255495\nEpoch 15, Batch 500, Loss: 0.244226\nEpoch 15, Batch 600, Loss: 0.258844\nEpoch 15, Batch 700, Loss: 0.232443\nEpoch 15, Batch 800, Loss: 0.245170\nEpoch 15, Batch 900, Loss: 0.269942\nEpoch 15, Batch 1000, Loss: 0.240608\nEpoch 15, Batch 1100, Loss: 0.237907\nEpoch 15, Batch 1200, Loss: 0.249175\nEpoch 15, Batch 1300, Loss: 0.234508\nEpoch 15, Batch 1400, Loss: 0.239582\nEpoch 15, Batch 1500, Loss: 0.232217\nEpoch 15, Batch 1600, Loss: 0.275038\nloss: 0.007466, Accuracy: 91.33%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 16, Batch 100, Loss: 0.244888\nEpoch 16, Batch 200, Loss: 0.225182\nEpoch 16, Batch 300, Loss: 0.239179\nEpoch 16, Batch 400, Loss: 0.245064\nEpoch 16, Batch 500, Loss: 0.233597\nEpoch 16, Batch 600, Loss: 0.246785\nEpoch 16, Batch 700, Loss: 0.248231\nEpoch 16, Batch 800, Loss: 0.246789\nEpoch 16, Batch 900, Loss: 0.268495\nEpoch 16, Batch 1000, Loss: 0.258923\nEpoch 16, Batch 1100, Loss: 0.283362\nEpoch 16, Batch 1200, Loss: 0.240890\nEpoch 16, Batch 1300, Loss: 0.256171\nEpoch 16, Batch 1400, Loss: 0.244702\nEpoch 16, Batch 1500, Loss: 0.253022\nEpoch 16, Batch 1600, Loss: 0.246763\nloss: 0.007668, Accuracy: 90.92%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 17, Batch 100, Loss: 0.223591\nEpoch 17, Batch 200, Loss: 0.230010\nEpoch 17, Batch 300, Loss: 0.252789\nEpoch 17, Batch 400, Loss: 0.254202\nEpoch 17, Batch 500, Loss: 0.236606\nEpoch 17, Batch 600, Loss: 0.236262\nEpoch 17, Batch 700, Loss: 0.247185\nEpoch 17, Batch 800, Loss: 0.255639\nEpoch 17, Batch 900, Loss: 0.253282\nEpoch 17, Batch 1000, Loss: 0.227441\nEpoch 17, Batch 1100, Loss: 0.252405\nEpoch 17, Batch 1200, Loss: 0.242529\nEpoch 17, Batch 1300, Loss: 0.262441\nEpoch 17, Batch 1400, Loss: 0.261534\nEpoch 17, Batch 1500, Loss: 0.236581\nEpoch 17, Batch 1600, Loss: 0.248210\nloss: 0.007600, Accuracy: 91.13%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 18, Batch 100, Loss: 0.239135\nEpoch 18, Batch 200, Loss: 0.247919\nEpoch 18, Batch 300, Loss: 0.226524\nEpoch 18, Batch 400, Loss: 0.230800\nEpoch 18, Batch 500, Loss: 0.239179\nEpoch 18, Batch 600, Loss: 0.244866\nEpoch 18, Batch 700, Loss: 0.253047\nEpoch 18, Batch 800, Loss: 0.245266\nEpoch 18, Batch 900, Loss: 0.237495\nEpoch 18, Batch 1000, Loss: 0.234433\nEpoch 18, Batch 1100, Loss: 0.248127\nEpoch 18, Batch 1200, Loss: 0.237947\nEpoch 18, Batch 1300, Loss: 0.232917\nEpoch 18, Batch 1400, Loss: 0.235277\nEpoch 18, Batch 1500, Loss: 0.242712\nEpoch 18, Batch 1600, Loss: 0.237301\nloss: 0.007512, Accuracy: 91.25%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 19, Batch 100, Loss: 0.218906\nEpoch 19, Batch 200, Loss: 0.242166\nEpoch 19, Batch 300, Loss: 0.234598\nEpoch 19, Batch 400, Loss: 0.217239\nEpoch 19, Batch 500, Loss: 0.252761\nEpoch 19, Batch 600, Loss: 0.238335\nEpoch 19, Batch 700, Loss: 0.242717\nEpoch 19, Batch 800, Loss: 0.238450\nEpoch 19, Batch 900, Loss: 0.244742\nEpoch 19, Batch 1000, Loss: 0.248496\nEpoch 19, Batch 1100, Loss: 0.229465\nEpoch 19, Batch 1200, Loss: 0.255422\nEpoch 19, Batch 1300, Loss: 0.235390\nEpoch 19, Batch 1400, Loss: 0.236536\nEpoch 19, Batch 1500, Loss: 0.229716\nEpoch 19, Batch 1600, Loss: 0.233418\nloss: 0.007246, Accuracy: 91.38%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nEpoch 20, Batch 100, Loss: 0.242031\nEpoch 20, Batch 200, Loss: 0.221100\nEpoch 20, Batch 300, Loss: 0.242052\nEpoch 20, Batch 400, Loss: 0.245763\nEpoch 20, Batch 500, Loss: 0.239099\nEpoch 20, Batch 600, Loss: 0.240996\nEpoch 20, Batch 700, Loss: 0.233973\nEpoch 20, Batch 800, Loss: 0.223704\nEpoch 20, Batch 900, Loss: 0.241720\nEpoch 20, Batch 1000, Loss: 0.219825\nEpoch 20, Batch 1100, Loss: 0.225594\nEpoch 20, Batch 1200, Loss: 0.227201\nEpoch 20, Batch 1300, Loss: 0.232950\nEpoch 20, Batch 1400, Loss: 0.228721\nEpoch 20, Batch 1500, Loss: 0.210683\nEpoch 20, Batch 1600, Loss: 0.244072\nloss: 0.007281, Accuracy: 91.37%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\nTraining complete!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model.load_state_dict(torch.load('best_model.pth'))\nprint(\"test performance metrics\")\ntest_loss, test_accuracy = validate(model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:13:56.829313Z","iopub.execute_input":"2025-02-16T12:13:56.829674Z","iopub.status.idle":"2025-02-16T12:13:59.152944Z","shell.execute_reply.started":"2025-02-16T12:13:56.829643Z","shell.execute_reply":"2025-02-16T12:13:59.152072Z"}},"outputs":[{"name":"stdout","text":"test performance metrics\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-11-b4ea9fd60318>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"loss: 0.007499, Accuracy: 91.39%\nPrecision: 0.91\nRecall: 0.91\nF1 Score: 0.91\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}